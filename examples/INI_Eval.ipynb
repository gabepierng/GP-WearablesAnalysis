{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The module is located at: c:\\GP-WearablesAnalysis\\examples\\excel_reader_gcp.py\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import scipy.interpolate as interp\n",
    "sys.path.append(os.path.join(sys.path[0], '..', 'src'))\n",
    "import excel_reader_gcp as excel_reader\n",
    "import excel_reader_gcp_GN as excel_reader_GN\n",
    "print(\"The module is located at:\", excel_reader.__file__)\n",
    "import datetime\n",
    "import logging   \n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import gait_metrics as gait_metrics\n",
    "from gait_metrics import *\n",
    "\n",
    "\n",
    "\n",
    "def reshape_vector(vectors_orig, new_size=40, num_axes=3):\n",
    "    x_new = np.linspace(0, 100, new_size)\n",
    "    trial_reshaped = []\n",
    "    for stride in vectors_orig:\n",
    "        x_orig = np.linspace(0, 100, len(stride))\n",
    "        func_cubic = [interp.interp1d(x_orig, stride[:, i], kind='cubic') for i in range(num_axes)]\n",
    "        vec_cubic = np.array([func_cubic[i](x_new) for i in range(num_axes)]).transpose()\n",
    "        trial_reshaped.append(vec_cubic)\n",
    "    return np.array(trial_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_groupings(num_groups, gait_parameter, gait_cycles, part_strides, spatiotemp, percent_grading, reverse=True):\n",
    "\n",
    "    # (Same code as before until we need to handle part_strides)\n",
    "    if reverse:\n",
    "        percent_grading = -percent_grading\n",
    "        values_sorted = sorted(gait_parameter, reverse=True)\n",
    "        sorted_indices = np.argsort(gait_parameter)[::-1]  # Sort indices in descending order of stance time symmetry\n",
    "    else:\n",
    "        sorted_indices = np.argsort(gait_parameter)\n",
    "        values_sorted = sorted(gait_parameter, reverse=False)\n",
    "\n",
    "    n = len(gait_parameter)\n",
    "    group_sizes = [n // num_groups + (1 if i < n % num_groups else 0) for i in range(num_groups)]\n",
    "    target_means = [np.mean(values_sorted[:group_sizes[0]])]\n",
    "\n",
    "    # Initializes the groups and the remaining values to be picked from\n",
    "    groups = [[] for _ in range(num_groups)]\n",
    "    grouped_gait_cycles = [[] for _ in range(num_groups)]\n",
    "    remaining_indices = sorted_indices[:]\n",
    "\n",
    "    for i in range(1, num_groups):\n",
    "        target_means.append(target_means[0] + percent_grading * i)\n",
    "\n",
    "    # Initializes the groups and the remaining values to be picked from\n",
    "    groups = [[] for _ in range(num_groups)]\n",
    "    grouped_gait_cycles = [[] for _ in range(num_groups)]\n",
    "    grouped_strides_dict = [{} for _ in range(num_groups)]\n",
    "    grouped_spatiotemp_dict = [{} for _ in range(num_groups)]\n",
    "     \n",
    "    remaining_indices = sorted_indices[:]\n",
    "\n",
    "    for i in range(1, num_groups):\n",
    "        target_means.append(target_means[0] + percent_grading * i)\n",
    "\n",
    "    params = spatiotemp[trial_type]\n",
    "    \n",
    "    for i in range(num_groups):\n",
    "        target_mean = target_means[i]\n",
    "        filtered_indices = [idx for idx in remaining_indices if abs(gait_parameter[idx] - target_mean) < percent_grading / 2]\n",
    "        selected_indices = filtered_indices[:]\n",
    "        \n",
    "        groups[i].extend(gait_parameter[idx] for idx in selected_indices)\n",
    "        grouped_gait_cycles[i].extend(gait_cycles[idx] for idx in selected_indices)\n",
    "        \n",
    "        for key, data in part_strides[trial_type].items():\n",
    "            # Extract the desired indices from each array\n",
    "            selected_data = []\n",
    "            for array in data:\n",
    "                # Check if array is long enough\n",
    "                if len(array) > max(selected_indices):\n",
    "                    selected_values = [array[i] for i in selected_indices if i < len(array)]\n",
    "                    selected_data.append(selected_values)\n",
    "            grouped_strides_dict[i][key] = selected_data\n",
    "            \n",
    "        subsampled_params = []\n",
    "        for param in params:\n",
    "            subsampled_param = param[:, selected_indices]\n",
    "            subsampled_params.append(subsampled_param)\n",
    "    \n",
    "        # Add the subsampled arrays to the new dictionary\n",
    "        grouped_spatiotemp_dict[i] = subsampled_params\n",
    "        \n",
    "        remaining_indices = [idx for idx in remaining_indices if idx not in selected_indices]\n",
    "            \n",
    "    return groups, grouped_gait_cycles, grouped_strides_dict, grouped_spatiotemp_dict\n",
    "\n",
    "def random_sampling(groups, grouped_gait_cycles, grouped_strides_dict, grouped_spatiotemp_dict, sample_size=50):\n",
    "    def adaptive_subsample(group, first_mean, i, percent_grading=0.03, tolerance=0.005, sample_size=50, max_iterations=10000):\n",
    "        available_indices = list(range(len(group)))  # List of all indices in the group\n",
    "        sample_indices = np.random.choice(available_indices, size=sample_size, replace=False)\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            available_indices.remove(idx)  # Remove initial sample values from available values\n",
    "        \n",
    "        for _ in range(max_iterations):\n",
    "            current_mean = np.mean([group[idx] for idx in sample_indices])\n",
    "            percent_diff = current_mean - first_mean \n",
    "            target_diff = percent_grading * i\n",
    "\n",
    "            if len(available_indices) == 0:\n",
    "                raise ValueError(\"No candidates available to adjust the mean\")\n",
    "            \n",
    "            if (target_diff - tolerance) <= abs(percent_diff) <= (target_diff + tolerance):\n",
    "                return sample_indices\n",
    "            \n",
    "            if abs(percent_diff) < (target_diff - tolerance):\n",
    "                if percent_diff < 0:\n",
    "                    # Choose a new sample from the lower half\n",
    "                    lower_idx = [idx for idx in available_indices if group[idx] <= np.percentile(group, 50)]\n",
    "                    if lower_idx:\n",
    "                        new_idx = np.random.choice(lower_idx)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    else:\n",
    "                        new_idx = np.random.choice(available_indices)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    \n",
    "                    available_indices.remove(new_idx)\n",
    "                    sample_indices = np.delete(sample_indices, np.argmax([group[idx] for idx in sample_indices]))\n",
    "                else:\n",
    "                    # Choose a new sample from the upper half\n",
    "                    higher_idx = [idx for idx in available_indices if group[idx] >= np.percentile(group, 50)]\n",
    "                    if higher_idx:\n",
    "                        new_idx = np.random.choice(higher_idx)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    else:\n",
    "                        new_idx = np.random.choice(available_indices)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    \n",
    "                    available_indices.remove(new_idx)\n",
    "                    sample_indices = np.delete(sample_indices, np.argmin([group[idx] for idx in sample_indices]))\n",
    "            else:\n",
    "                if percent_diff > 0:\n",
    "                    lower_idx = [idx for idx in available_indices if group[idx] <= np.percentile(group, 50)]\n",
    "                    if lower_idx:\n",
    "                        new_idx = np.random.choice(lower_idx)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    else:\n",
    "                        new_idx = np.random.choice(available_indices)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    \n",
    "                    available_indices.remove(new_idx)\n",
    "                    sample_indices = np.delete(sample_indices, np.argmax([group[idx] for idx in sample_indices]))\n",
    "                else:\n",
    "                    # Choose a new sample from the upper half\n",
    "                    higher_idx = [idx for idx in available_indices if group[idx] >= np.percentile(group, 50)]\n",
    "                    if higher_idx:\n",
    "                        new_idx = np.random.choice(higher_idx)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    else:\n",
    "                        new_idx = np.random.choice(available_indices)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    \n",
    "                    available_indices.remove(new_idx)\n",
    "                    sample_indices = np.delete(sample_indices, np.argmin([group[idx] for idx in sample_indices]))\n",
    "\n",
    "        raise ValueError(\"Could not find suitable subsample within the maximum number of iterations\")\n",
    "\n",
    "    groups_subsampled_list = []\n",
    "    gaitcycles_subsampled_list = []\n",
    "    \n",
    "    # Sample for the first group\n",
    "    indices_first_group = list(range(len(groups[0])))\n",
    "    sample_indices_first_group = np.random.choice(indices_first_group, size=sample_size * 2, replace=False)\n",
    "    group1_mean = np.mean([groups[0][idx] for idx in sample_indices_first_group])  # First mean used as the target for all subsequent groups\n",
    "\n",
    "    random.shuffle(sample_indices_first_group)\n",
    "    \n",
    "    # ###REMOVE LATER\n",
    "    \n",
    "    grouped_strides_dict_subsampled = [{} for _ in range(4)]\n",
    "    grouped_spatiotemp_dict_subsampled = [{} for _ in range(4)]\n",
    "    \n",
    "    # for key, data in grouped_strides_dict[0].items():\n",
    "    #     selected_data = []\n",
    "    #     for array in data:\n",
    "    #         # Check if array is long enough\n",
    "    #         if len(array) > max(sample_indices_first_group):\n",
    "    #             selected_values = [array[i] for i in sample_indices_first_group if i < len(array)]\n",
    "    #             selected_data.append(selected_values)\n",
    "    #     grouped_strides_dict_subsampled[0][key] = selected_data\n",
    "    \n",
    "    # params = grouped_spatiotemp_dict[0]\n",
    "    \n",
    "    # subsampled_params = []\n",
    "    # for param in params:\n",
    "    #     subsampled_param = param[:, sample_indices_first_group]\n",
    "    #     subsampled_params.append(subsampled_param)\n",
    "    # # Add the subsampled arrays to the new dictionary\n",
    "    # grouped_spatiotemp_dict_subsampled[0] = subsampled_params    \n",
    "        \n",
    "    # subsampled_values_FULLBASELINE = [groups[0][j] for j in sample_indices_first_group]\n",
    "    # subsampled_gait_cycles_FULLBASELINE = [grouped_gait_cycles[0][j] for j in sample_indices_first_group]\n",
    "    \n",
    "    # groups_subsampled_list.append(subsampled_values_FULLBASELINE)\n",
    "    # gaitcycles_subsampled_list.append(subsampled_gait_cycles_FULLBASELINE)\n",
    "    \n",
    "\n",
    "    baseline_1_indices = sample_indices_first_group[:50]\n",
    "    baseline_2_indices = sample_indices_first_group[50:]\n",
    "\n",
    "    subsampled_values_baseline1 = [groups[0][j] for j in baseline_1_indices]\n",
    "    subsampled_gait_cycles_baseline1 = [grouped_gait_cycles[0][j] for j in baseline_1_indices]\n",
    "\n",
    "    subsampled_values_baseline2 = [groups[0][j] for j in baseline_2_indices]\n",
    "    subsampled_gait_cycles_baseline2 = [grouped_gait_cycles[0][j] for j in baseline_2_indices]\n",
    "\n",
    "    groups_subsampled_list.append(subsampled_values_baseline1)\n",
    "    groups_subsampled_list.append(subsampled_values_baseline2)\n",
    "\n",
    "    gaitcycles_subsampled_list.append(subsampled_gait_cycles_baseline1)\n",
    "    gaitcycles_subsampled_list.append(subsampled_gait_cycles_baseline2)\n",
    "\n",
    "    \n",
    "    for key, data in grouped_strides_dict[0].items():\n",
    "        selected_data = []\n",
    "        for array in data:\n",
    "            # Check if array is long enough\n",
    "            if len(array) > max(baseline_1_indices):\n",
    "                selected_values = [array[i] for i in baseline_1_indices if i < len(array)]\n",
    "                selected_data.append(selected_values)\n",
    "        grouped_strides_dict_subsampled[0][key] = selected_data\n",
    "    \n",
    "    for key, data in grouped_strides_dict[0].items():\n",
    "        selected_data = []\n",
    "        for array in data:\n",
    "            # Check if array is long enough\n",
    "            if len(array) > max(baseline_2_indices):\n",
    "                selected_values = [array[i] for i in baseline_2_indices if i < len(array)]\n",
    "                selected_data.append(selected_values)\n",
    "        grouped_strides_dict_subsampled[1][key] = selected_data  \n",
    "        \n",
    "    params = grouped_spatiotemp_dict[0]\n",
    "    \n",
    "    subsampled_params = []\n",
    "    for param in params:\n",
    "        subsampled_param = param[:, baseline_1_indices]\n",
    "        subsampled_params.append(subsampled_param)\n",
    "    # Add the subsampled arrays to the new dictionary\n",
    "    grouped_spatiotemp_dict_subsampled[0] = subsampled_params\n",
    "    \n",
    "    subsampled_params = []\n",
    "    for param in params:\n",
    "        subsampled_param = param[:, baseline_2_indices]\n",
    "        subsampled_params.append(subsampled_param)\n",
    "    # Add the subsampled arrays to the new dictionary\n",
    "    grouped_spatiotemp_dict_subsampled[1] = subsampled_params\n",
    "    \n",
    "    # Only consider 3 groups here\n",
    "    for i in range(1, 3):\n",
    "        sample_indices = adaptive_subsample(np.array(groups[i]), group1_mean, i)\n",
    "        subsampled_values = [groups[i][j] for j in sample_indices]\n",
    "        subsampled_gait_cycles = [grouped_gait_cycles[i][j] for j in sample_indices]\n",
    "        groups_subsampled_list.append(subsampled_values)\n",
    "        gaitcycles_subsampled_list.append(subsampled_gait_cycles)\n",
    "        \n",
    "        for key, data in grouped_strides_dict[i].items():\n",
    "            selected_data = []\n",
    "            for array in data:\n",
    "                # Check if array is long enough\n",
    "                if len(array) > max(sample_indices):\n",
    "                    selected_values = [array[i] for i in sample_indices if i < len(array)]\n",
    "                    selected_data.append(selected_values)\n",
    "            \n",
    "            grouped_strides_dict_subsampled[i+1][key] = selected_data  \n",
    "        \n",
    "        params = grouped_spatiotemp_dict[i]    \n",
    "        subsampled_params = []\n",
    "        for param in params:\n",
    "            subsampled_param = param[:, sample_indices]\n",
    "            subsampled_params.append(subsampled_param)\n",
    "        # Add the subsampled arrays to the new dictionary\n",
    "        grouped_spatiotemp_dict_subsampled[i+1] = subsampled_params\n",
    "                \n",
    "    return groups_subsampled_list, gaitcycles_subsampled_list, grouped_strides_dict_subsampled, grouped_spatiotemp_dict_subsampled\n",
    "\n",
    "\n",
    "def check_group_configurations(gait_split_parameter, raw_sensor_data, part_strides, spatiotemp):\n",
    "    percent_grading = 0.03\n",
    "    groups, grouped_gait_cycles, grouped_strides_dict, grouped_spatiotemp_dict = finding_groupings(4, gait_split_parameter, raw_sensor_data, part_strides, spatiotemp, percent_grading, reverse=False)\n",
    "    \n",
    "    filtered_groups = []\n",
    "    filtered_gait_groups = []\n",
    "    filtered_strides = []\n",
    "    filtered_spatiotemp = []\n",
    "    \n",
    "    for i in range(len(groups)):\n",
    "        if len(groups[i]) > 70:\n",
    "            filtered_groups.append(groups[i])\n",
    "            filtered_gait_groups.append(grouped_gait_cycles[i])\n",
    "            filtered_strides.append(grouped_strides_dict[i])\n",
    "            filtered_spatiotemp.append(grouped_spatiotemp_dict[i])\n",
    "    \n",
    "    if len(filtered_groups) < 3:\n",
    "        groups, grouped_gait_cycles, grouped_strides_dict, grouped_spatiotemp_dict = finding_groupings(4, gait_split_parameter, raw_sensor_data, part_strides, spatiotemp, percent_grading, reverse=True)  # Try the other direction if requirements are not fulfilled\n",
    "        filtered_groups = []\n",
    "        filtered_gait_groups = []\n",
    "        filtered_strides = []\n",
    "        filtered_spatiotemp = []\n",
    "        \n",
    "        for i in range(len(groups)):\n",
    "            if len(groups[i]) > 70:\n",
    "                filtered_groups.append(groups[i])\n",
    "                filtered_gait_groups.append(grouped_gait_cycles[i])\n",
    "                filtered_strides.append(grouped_strides_dict[i])\n",
    "                filtered_spatiotemp.append(grouped_spatiotemp_dict[i])\n",
    "    \n",
    "        if len(filtered_groups) < 3:\n",
    "            raise ValueError(\"Insufficient group sizes available for this participant\")\n",
    "    \n",
    "    groups, gaitcycles, strides, spatiotemp = random_sampling(filtered_groups, filtered_gait_groups, filtered_strides, filtered_spatiotemp)\n",
    "    \n",
    "    return groups, gaitcycles, strides, spatiotemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#Establishing control for GPS\n",
    "\n",
    "storage_client = storage.Client()\n",
    "part_strides = {}\n",
    "part_gait_params = {}\n",
    "part_kinematic_params = {}\n",
    "control_strides = {}\n",
    "control_gait_params = {}\n",
    "control_kinematic_params = {}\n",
    "\n",
    "\n",
    "bucket_dir = 'gs://gaitbfb_propellab/'\n",
    "\n",
    "\n",
    "def reshape_vector(vectors_orig, new_size=40, num_axes=3):\n",
    "    x_new = np.linspace(0, 100, new_size)\n",
    "    trial_reshaped = []\n",
    "    for stride in vectors_orig:\n",
    "        x_orig = np.linspace(0, 100, len(stride))\n",
    "        func_cubic = [interp.interp1d(x_orig, stride[:, i], kind='cubic') for i in range(num_axes)]\n",
    "        vec_cubic = np.array([func_cubic[i](x_new) for i in range(num_axes)]).transpose()\n",
    "        trial_reshaped.append(vec_cubic)\n",
    "    return np.array(trial_reshaped)\n",
    "\n",
    "def compile_gait_data(store_gait_cycles, store_gait_params, store_kin_params, filenames, trial_type_filter, print_filenames=False, look_at_all_files = True, desired_filetypes=None):   \n",
    "\n",
    "    XsensGaitParser = excel_reader_GN.XsensGaitDataParser()  \n",
    "    for i, file in enumerate(sorted(filenames)):\n",
    "        trial_type = re.search(trial_type_filter, file).group(1)\n",
    "        if(look_at_all_files or any(filetype in file for filetype in desired_filetypes)):\n",
    "            XsensGaitParser.process_mvn_trial_data(os.path.join(bucket_dir, file))\n",
    "            partitioned_mvn_data = XsensGaitParser.get_partitioned_mvn_data()\n",
    "            gait_params = XsensGaitParser.get_gait_param_info()\n",
    "\n",
    "            if trial_type in store_gait_cycles:\n",
    "                for body_part in store_gait_cycles[trial_type]:\n",
    "                    for i, side in enumerate(store_gait_cycles[trial_type][body_part]):\n",
    "                        # for each part (pelvis, l_hip, r_knee, etc.), append strides to appropriate list\n",
    "                        store_gait_cycles[trial_type][body_part][i] = store_gait_cycles[trial_type][body_part][i] + partitioned_mvn_data[body_part][i]\n",
    "\n",
    "                store_gait_params[trial_type].append(gait_params['spatio_temp'])\n",
    "\n",
    "                for joint in store_kin_params[trial_type]:\n",
    "                    for i, side in enumerate(store_kin_params[trial_type][joint]):\n",
    "                        store_kin_params[trial_type][joint][i] = np.append(store_kin_params[trial_type][joint][i], gait_params['kinematics'][joint][i], axis=0)\n",
    "\n",
    "            else:\n",
    "                store_gait_cycles[trial_type] = partitioned_mvn_data\n",
    "                store_gait_params[trial_type] = [gait_params['spatio_temp']]\n",
    "                store_kin_params[trial_type] = gait_params['kinematics']\n",
    "\n",
    "\n",
    "bucket_name = 'gaitbfb_propellab/'\n",
    "base_directory = bucket_name + 'Wearable Biofeedback System (REB-0448)/Data/Raw Data'\n",
    "bucket_name = 'gaitbfb_propellab'\n",
    "prefix = 'control_dir'\n",
    "control_dir = 'Gait Quality Analysis/Data/Participant_Data/Processed Data/AbleBodied_Control/CSV'\n",
    "blobs = storage_client.list_blobs(bucket_name, prefix = control_dir)\n",
    "control_files = []\n",
    "for blob in blobs:\n",
    "    if('.csv' in blob.name):\n",
    "        control_files.append(blob.name)\n",
    "\n",
    "compile_gait_data(control_strides, control_gait_params, control_kinematic_params, control_files, 'CSV/(.*?)-00')        \n",
    "\n",
    "aggregate_control_data = {}\n",
    "strides_per_control = 10\n",
    "for i, indiv in enumerate(control_strides.keys()):\n",
    "    indices = np.arange(len(control_strides[indiv]['gyro_data'][0]))\n",
    "    np.random.shuffle(indices)\n",
    "    #control_strides_per_part.append(min(strides_per_control, len(indices)))\n",
    "    \n",
    "    if(i == 0):\n",
    "        aggregate_control_data = control_strides[indiv]\n",
    "        \n",
    "        for signal_type in control_strides[indiv]:\n",
    "            for j, side in enumerate(control_strides[indiv][signal_type]):\n",
    "                aggregate_control_data[signal_type][j] = [control_strides[indiv][signal_type][j][indices[k]] for k in range(min(strides_per_control, len(indices))) ]\n",
    "                                    \n",
    "    else:\n",
    "        # randomly sample 10 gait cycles from each able-bodied in control, or all gait cycles if less than 10\n",
    "        for signal_type in control_strides[indiv]:\n",
    "            for j, side in enumerate(control_strides[indiv][signal_type]):\n",
    "                aggregate_control_data[signal_type][j] = aggregate_control_data[signal_type][j] + [control_strides[indiv][signal_type][j][indices[k]] \n",
    "                                                                                                for k in range(min(strides_per_control, len(indices))) ]\n",
    "\n",
    "# reshape all the kinematic signals to the size specified for the GPS (51, e.g. 2% increments across the gait cycles from HS to HS)\n",
    "# store in partitioned_awinda_control\n",
    "partitioned_awinda_control = {}\n",
    "partitioned_awinda_control['pelvis_orient'] = reshape_vector(aggregate_control_data['pelvis_orient'][0], new_size = 51)\n",
    "partitioned_awinda_control['hip_angle'] = [reshape_vector(aggregate_control_data['hip_angle'][0], new_size = 51), reshape_vector(aggregate_control_data['hip_angle'][1], new_size = 51)]\n",
    "partitioned_awinda_control['knee_angle'] = [reshape_vector(aggregate_control_data['knee_angle'][0], new_size = 51), reshape_vector(aggregate_control_data['knee_angle'][1], new_size = 51)]\n",
    "partitioned_awinda_control['ankle_angle'] = [reshape_vector(aggregate_control_data['ankle_angle'][0], new_size = 51), reshape_vector(aggregate_control_data['ankle_angle'][1], new_size = 51)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed control data...\n"
     ]
    }
   ],
   "source": [
    "### USING AB AS CONTROL####\n",
    "\n",
    "XsensGaitParser =  excel_reader_GN.XsensGaitDataParser()\n",
    "storage_client = storage.Client()\n",
    "part_strides = {}\n",
    "part_gait_params = {}\n",
    "part_kinematic_params = {}\n",
    "control_strides = {}\n",
    "control_gait_params = {}\n",
    "control_kinematic_params = {}\n",
    "\n",
    "params_INI = {\n",
    "            'stride time':[],\n",
    "            'stride length':[],\n",
    "            'swing phase':[],\n",
    "            'MAV':[],\n",
    "            'MAH':[],\n",
    "            'MHD':[],\n",
    "            'MAB':[],\n",
    "            'MAD':[],\n",
    "            'ShROM':[]\n",
    "             }\n",
    "\n",
    "ini_height_norm = {\n",
    "            'stride time':False,\n",
    "            'stride length':True,\n",
    "            'swing phase':False,\n",
    "            'MAV':True,\n",
    "            'MAH':True,\n",
    "            'MHD':True,\n",
    "            'MAB':True,\n",
    "            'MAD':True,\n",
    "            'ShROM':False\n",
    "            }\n",
    "params_temporal_mgs = {\n",
    "            'stance time':[],\n",
    "            'swing time':[],\n",
    "            'double support':[],\n",
    "            'step time':[],\n",
    "            'stride time':[]\n",
    "            }\n",
    "\n",
    "bucket_dir = 'gs://gaitbfb_propellab/'\n",
    "def compile_gait_data(store_gait_cycles, store_gait_params, store_kin_params, filenames, trial_type_filter, print_filenames=False, look_at_all_files = True, desired_filetypes=None):   \n",
    " \n",
    "    XsensGaitParser = excel_reader_GN.XsensGaitDataParser()  \n",
    "    for i, file in enumerate(sorted(filenames)):\n",
    "        trial_type = re.search(trial_type_filter, file).group(1)\n",
    "\n",
    "        if(look_at_all_files or any(filetype in file for filetype in desired_filetypes)):\n",
    "            XsensGaitParser.process_mvn_trial_data(os.path.join(bucket_dir, file))\n",
    "            partitioned_mvn_data = XsensGaitParser.get_partitioned_mvn_data()\n",
    "            gait_params = XsensGaitParser.get_gait_param_info()\n",
    "\n",
    "            if trial_type in store_gait_cycles:\n",
    "                for body_part in store_gait_cycles[trial_type]:\n",
    "                    for j, side in enumerate(store_gait_cycles[trial_type][body_part]):\n",
    "                        # for each part (pelvis, l_hip, r_knee, etc.), append strides to appropriate list\n",
    "                        store_gait_cycles[trial_type][body_part][j] = store_gait_cycles[trial_type][body_part][j] + partitioned_mvn_data[body_part][j]\n",
    "\n",
    "                for j, param in enumerate(store_gait_params[trial_type]):\n",
    "                    store_gait_params[trial_type][j] = np.concatenate( (store_gait_params[trial_type][j], gait_params['spatio_temp'][j]), axis=-1)\n",
    "\n",
    "                for joint in store_kin_params[trial_type]:\n",
    "                    for j, side in enumerate(store_kin_params[trial_type][joint]):\n",
    "                        store_kin_params[trial_type][joint][j] = np.append(store_kin_params[trial_type][joint][j], gait_params['kinematics'][joint][j], axis=0)\n",
    "\n",
    "            else:\n",
    "                store_gait_cycles[trial_type] = partitioned_mvn_data\n",
    "                store_gait_params[trial_type] = gait_params['spatio_temp']\n",
    "                store_kin_params[trial_type] = gait_params['kinematics']\n",
    "                \n",
    "                \n",
    "bucket_name = 'gaitbfb_propellab/'\n",
    "base_directory = bucket_name + 'Wearable Biofeedback System (REB-0448)/Data/Raw Data'\n",
    "bucket_name = 'gaitbfb_propellab'\n",
    "prefix = 'control_dir'\n",
    "control_dir = 'Gait Quality Analysis/Data/Participant_Data/Processed Data/AbleBodied_Control/CSV'\n",
    "blobs = storage_client.list_blobs(bucket_name, prefix = control_dir)\n",
    "control_files = []\n",
    "for blob in blobs:\n",
    "    if('.csv' in blob.name):\n",
    "        control_files.append(blob.name)\n",
    "participant_info = pd.read_csv(bucket_dir + 'Gait Quality Analysis/Data/Participant_Data/Raw Data/participant_info.csv')\n",
    "compile_gait_data(control_strides, control_gait_params, control_kinematic_params, control_files, 'CSV/(.*?)-00')        \n",
    "\n",
    "aggregate_control_data = {}\n",
    "strides_per_control = 10\n",
    "control_strides_per_part = []\n",
    "for i, indiv in enumerate(control_strides.keys()):\n",
    "    \n",
    "    indiv_height = participant_info.loc[participant_info['Participant'] == indiv]['Height (m)'].item()\n",
    "    for j, param in enumerate(params_INI):\n",
    "        if(ini_height_norm[param]):\n",
    "            control_gait_params[indiv][j] = control_gait_params[indiv][j] / indiv_height  \n",
    "\n",
    "    indices = np.arange(len(control_strides[indiv]['gyro_data'][0]))\n",
    "    np.random.shuffle(indices)\n",
    "    control_strides_per_part.append(min(strides_per_control, len(indices)))\n",
    "    \n",
    "    if(i == 0):\n",
    "        aggregate_control_data = control_strides[indiv]\n",
    "        \n",
    "        for signal_type in control_strides[indiv]:\n",
    "            for j, side in enumerate(control_strides[indiv][signal_type]):\n",
    "                aggregate_control_data[signal_type][j] = [control_strides[indiv][signal_type][j][indices[k]] for k in range(min(strides_per_control, len(indices))) ]\n",
    "                    \n",
    "        for j, param in enumerate(params_INI.keys()):\n",
    "            for k in range(2):\n",
    "                params_INI[param].append([])\n",
    "                params_INI[param][k] = [control_gait_params[indiv][j][k][indices[a]] for a in range(min(strides_per_control, len(indices))) ]\n",
    "        \n",
    "    else:\n",
    "        # choose 10 gait cycles randomly from each able-bodied participant, or all gait cycles if less than 10\n",
    "        for signal_type in control_strides[indiv]:\n",
    "            for j, side in enumerate(control_strides[indiv][signal_type]):\n",
    "                aggregate_control_data[signal_type][j] = aggregate_control_data[signal_type][j] + [control_strides[indiv][signal_type][j][indices[k]] \n",
    "                                                                                                   for k in range(min(strides_per_control, len(indices))) ]\n",
    "        for j, param in enumerate(params_INI.keys()):\n",
    "            for k in range(2):\n",
    "                params_INI[param][k] = params_INI[param][k] + [control_gait_params[indiv][j][k][indices[a]] for a in range(min(strides_per_control, len(indices))) ]\n",
    "                \n",
    "\n",
    "print('Parsed control data...')\n",
    "for param in params_INI:\n",
    "    params_INI[param] = np.array(params_INI[param])\n",
    "    \n",
    "class AwindaData():\n",
    "    def __init__(self, trial_type, partitioned_movement_data, ini_gait_params = None, mgs_temporal_params = None):\n",
    "        self.trial_type = trial_type\n",
    "        self.partitioned_movement_data = partitioned_movement_data\n",
    "        self.ini_gait_params = ini_gait_params\n",
    "\n",
    "def combine_acc_and_gyro(numpy_acc, numpy_gyro):\n",
    "    return np.concatenate((numpy_acc, numpy_gyro), axis=-1)\n",
    "\n",
    "acc_scale = 0.02\n",
    "partitioned_awinda_control = {}\n",
    "partitioned_awinda_control['Pelvis_IMU'] = combine_acc_and_gyro(acc_scale*reshape_vector(aggregate_control_data['acc_data'][1]), reshape_vector(aggregate_control_data['gyro_data'][1]))\n",
    "partitioned_awinda_control['UpperR_IMU'] = combine_acc_and_gyro(acc_scale*reshape_vector(aggregate_control_data['acc_data'][2]), reshape_vector(aggregate_control_data['gyro_data'][2]))\n",
    "partitioned_awinda_control['UpperL_IMU'] = combine_acc_and_gyro(acc_scale*reshape_vector(aggregate_control_data['acc_data'][5]), reshape_vector(aggregate_control_data['gyro_data'][5]))\n",
    "partitioned_awinda_control['LowerR_IMU'] = combine_acc_and_gyro(acc_scale*reshape_vector(aggregate_control_data['acc_data'][3]), reshape_vector(aggregate_control_data['gyro_data'][3]))\n",
    "partitioned_awinda_control['LowerL_IMU'] = combine_acc_and_gyro(acc_scale*reshape_vector(aggregate_control_data['acc_data'][6]), reshape_vector(aggregate_control_data['gyro_data'][6]))\n",
    "\n",
    "partitioned_awinda_control['pelvis_orient'] = reshape_vector(aggregate_control_data['pelvis_orient'][0], new_size = 51)\n",
    "partitioned_awinda_control['hip_angle'] = [reshape_vector(aggregate_control_data['hip_angle'][0], new_size = 51), reshape_vector(aggregate_control_data['hip_angle'][1], new_size = 51)]\n",
    "partitioned_awinda_control['knee_angle'] = [reshape_vector(aggregate_control_data['knee_angle'][0], new_size = 51), reshape_vector(aggregate_control_data['knee_angle'][1], new_size = 51)]\n",
    "partitioned_awinda_control['ankle_angle'] = [reshape_vector(aggregate_control_data['ankle_angle'][0], new_size = 51), reshape_vector(aggregate_control_data['ankle_angle'][1], new_size = 51)]\n",
    "\n",
    "partitioned_control_data = AwindaData('control', dict(partitioned_awinda_control), dict(params_INI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\ekuep\\AppData\\Local\\Temp\\ipykernel_18256\\923435260.py:3: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  script_dir = 'C:\\GP-WearablesAnalysis\\examples'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing participant LLPU_P01\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P01\n",
      "5.873208017055056\n",
      "5.815506466500247\n",
      "5.708864165392883\n",
      "5.698261294251499\n",
      "Processing participant LLPU_P02\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P02\n",
      "6.227523637695038\n",
      "6.217045268685663\n",
      "5.859681886947804\n",
      "5.523357262568108\n",
      "Processing participant LLPU_P03\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P03\n",
      "4.771827753173785\n",
      "4.88411772033408\n",
      "4.753368911571264\n",
      "4.667817996693739\n",
      "Processing participant LLPU_P04\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P04\n",
      "5.614936171129435\n",
      "5.3792101883981305\n",
      "5.63325161950306\n",
      "5.575862334520312\n",
      "Processing participant LLPU_P05\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P05\n",
      "3.3574369980912695\n",
      "3.329748376055621\n",
      "3.3720692367923073\n",
      "3.3105897544557905\n",
      "Processing participant LLPU_P06\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P06\n",
      "6.041326870751351\n",
      "5.813686345145352\n",
      "6.033428616048163\n",
      "5.907386262864999\n",
      "Processing participant LLPU_P09\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P09\n",
      "4.022443409607957\n",
      "4.03493210703298\n",
      "4.187439731992509\n",
      "4.309127805826403\n",
      "Processing participant LLPU_P10\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P10\n",
      "3.965554432333143\n",
      "3.977859904091755\n",
      "3.9925034783890667\n",
      "3.906997150028143\n",
      "Processing participant LLPU_P12\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P12\n",
      "6.957130398987731\n",
      "6.902951033536852\n",
      "6.866119818588649\n",
      "6.859150759110594\n",
      "Processing participant LLPU_P15\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P15\n",
      "6.3796183983330605\n",
      "6.605879641300326\n",
      "6.333640492378605\n",
      "6.520684687781576\n"
     ]
    }
   ],
   "source": [
    "#Using AB as control\n",
    "\n",
    "script_dir = 'C:\\GP-WearablesAnalysis\\examples'\n",
    "run_time = datetime.datetime.now().strftime(\"%d-%m-%y_%H-%M\")\n",
    "csv_filename = f\"logresults_STSR_INI_ABControl_{run_time}.csv\" #Builds a log file based on the current time to keep track of runs\n",
    "csv_path = os.path.join(script_dir, csv_filename)\n",
    "\n",
    "def add_row_to_csv(csv_path, gait_param, algorithm, participant_num, level, parameter):\n",
    "    with open(csv_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([csv_path, gait_param, algorithm, participant_num, level, parameter])\n",
    "\n",
    "##### AGGREGATE TEST DATA AND EVAL INI #####\n",
    "\n",
    "XsensGaitParser =  excel_reader.XsensGaitDataParser() #Different excel reader file, which contains the spatiotemporal parameters not pertaining to the INI - used to be able to get stance time symmetry\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'gaitbfb_propellab/'\n",
    "base_directory = bucket_name + 'Wearable Biofeedback System (REB-0448)/Data/Raw Data'\n",
    "bucket_name = 'gaitbfb_propellab'\n",
    "blobs = storage_client.list_blobs(bucket_name, prefix = base_directory)\n",
    "prefix_from_bucket = 'Wearable Biofeedback System (REB-0448)/Data/Raw Data/' \n",
    "\n",
    "participant_list = ['LLPU_P01','LLPU_P02','LLPU_P03','LLPU_P04','LLPU_P05','LLPU_P06','LLPU_P09','LLPU_P10','LLPU_P12','LLPU_P15']\n",
    "arrangements = ['pelvis','upper','lower']\n",
    "\n",
    "def organize_signals(sensor_mappings, gyro_signal, accel_signal):\n",
    "    combined_signals = {}\n",
    "    for location, sensor in sensor_mappings.items():\n",
    "        reshaped_gyro = reshape_vector(gyro_signal[sensor], 40, 3)\n",
    "        reshaped_accel = reshape_vector(accel_signal[sensor], 40, 3)\n",
    "        combined_signals[location] = np.concatenate((reshaped_gyro, reshaped_accel), axis=2) #Concatenates to gyro x,y,z and accel x,y,z\n",
    "    return combined_signals\n",
    "\n",
    "def calc_INI(control_mean, control_std, eig_vals, eig_vecs, data):\n",
    "    standardized_data = (data - control_mean) / control_std\n",
    "    transformed_data = (np.dot(standardized_data, eig_vecs)) / np.sqrt(eig_vals)\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "#Dictionary to map the sensor locations to their IDs.\n",
    "sensor_mappings = {\n",
    "    'pelvis': 1,\n",
    "    'UpperR': 2,\n",
    "    'LowerR': 3,\n",
    "    'UpperL': 5,\n",
    "    'LowerL': 6\n",
    "}\n",
    "     \n",
    "for participant in participant_list:\n",
    "    print(f\"Processing participant {participant}\")\n",
    " \n",
    "    directory = prefix_from_bucket + participant + '/Excel_Data_Trimmed'\n",
    "    blobs = storage_client.list_blobs(bucket_or_name=bucket_name, prefix=directory.replace(\"\\\\\", \"/\"))\n",
    "    part_strides = {}\n",
    "    part_gait_params = {}\n",
    "    part_kinematic_params = {}\n",
    "    part_raw_sensor = []\n",
    "    trial_type = 'LLPU'\n",
    "    height_normalized = True\n",
    "    \n",
    "    part_gait_params_INI = {}\n",
    "    part_strides_INI = {}\n",
    "   \n",
    "    logging.info(f\"Processing participant {participant}\")\n",
    "    if blobs:\n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith('.csv'):\n",
    "                try:\n",
    "                    XsensGaitParser.process_mvn_trial_data(f\"gs://{bucket_name}/{blob.name}\")\n",
    "                    partitioned_mvn_data = XsensGaitParser.get_partitioned_mvn_data()\n",
    "                    gait_params = XsensGaitParser.get_gait_param_info()\n",
    "                    combined_signals = organize_signals(sensor_mappings, partitioned_mvn_data['gyro_data'], partitioned_mvn_data['acc_data'])\n",
    "                    pelvis_data = combined_signals['pelvis']\n",
    "                    upper_data = np.concatenate((combined_signals['UpperR'], combined_signals['UpperL']), axis=2)  # Concatenate by last axis\n",
    "                    lower_data = np.concatenate((combined_signals['LowerR'], combined_signals['LowerL']), axis=2)  # Concatenate by last axis\n",
    "                    full_sensors = np.concatenate((pelvis_data,upper_data,lower_data),axis=2)\n",
    "                    part_raw_sensor.append(full_sensors)\n",
    "\n",
    "                    if trial_type in part_strides:\n",
    "                        for body_part in part_strides[trial_type]:\n",
    "                            for i, side in enumerate(part_strides[trial_type][body_part]):\n",
    "                                # for each part (pelvis, l_hip, r_knee, etc.), append strides to appropriate list\n",
    "                                part_strides[trial_type][body_part][i] = part_strides[trial_type][body_part][i] + partitioned_mvn_data[body_part][i]\n",
    "                        \n",
    "                        part_gait_params[trial_type].append(gait_params['spatio_temp'])\n",
    "                        \n",
    "                        for joint in part_kinematic_params[trial_type]:\n",
    "                            for i, side in enumerate(part_kinematic_params[trial_type][joint]):\n",
    "                                part_kinematic_params[trial_type][joint][i] = np.append(part_kinematic_params[trial_type][joint][i], gait_params['kinematics'][joint][i], axis=0) \n",
    "\n",
    "                    else:\n",
    "                        part_strides[trial_type] = partitioned_mvn_data\n",
    "                        part_kinematic_params[trial_type] = gait_params['kinematics']\n",
    "                        part_gait_params[trial_type] = [gait_params['spatio_temp']]\n",
    "                    \n",
    "                    file_name = os.path.basename(blob.name)\n",
    "                    \n",
    "                    ##CHANGE THE excel reader - access the different \n",
    "                    XsensGaitParser_GN =  excel_reader_GN.XsensGaitDataParser()\n",
    "                    XsensGaitParser_GN.process_mvn_trial_data(f\"gs://{bucket_name}/{blob.name}\")\n",
    "                    partitioned_mvn_data_GN = XsensGaitParser_GN.get_partitioned_mvn_data()\n",
    "                    gait_params_GN = XsensGaitParser_GN.get_gait_param_info()\n",
    "                    \n",
    "                    if trial_type in part_strides_INI:\n",
    "                        for body_part in part_strides[trial_type]:\n",
    "                            for i, side in enumerate(part_strides_INI[trial_type][body_part]):\n",
    "                                # for each part (pelvis, l_hip, r_knee, etc.), append strides to appropriate list\n",
    "                                part_strides_INI[trial_type][body_part][i] = part_strides_INI[trial_type][body_part][i] + partitioned_mvn_data_GN[body_part][i]\n",
    "                        \n",
    "                        for j, param in enumerate(part_gait_params_INI[trial_type]):\n",
    "                            part_gait_params_INI[trial_type][j] = np.concatenate((part_gait_params_INI[trial_type][j], gait_params_GN['spatio_temp'][j]), axis=-1)\n",
    "\n",
    "                    else:\n",
    "                        part_strides_INI[trial_type] = partitioned_mvn_data_GN\n",
    "                        part_gait_params_INI[trial_type] = gait_params_GN['spatio_temp']\n",
    "                    \n",
    "\n",
    "                except IndexError as e: #Exception based on an Index Error encountered in excel_reader_gcp.py **\n",
    "                    #print(f\"File skipped: gs://{bucket_name}/{blob.name} due to error: {e}\")\n",
    "                    continue                              \n",
    "    \n",
    "    if trial_type in part_gait_params:\n",
    "        \n",
    "        stance_time_symmetry = [item for sublist in [i[11] for i in part_gait_params[trial_type]] for item in sublist]\n",
    "        \n",
    "        flattened_raw_sensor = []\n",
    "        part_strides_list = []\n",
    "        for sublist in part_raw_sensor:\n",
    "            for item in sublist:\n",
    "                flattened_raw_sensor.append(item) #Flatten to individual gait cycles\n",
    "        \n",
    "        partitioned_awinda_gait = {}\n",
    "        partitioned_awinda_gait['pelvis_orient'] = reshape_vector(part_strides[trial_type]['pelvis_orient'][0], new_size = 51)\n",
    "        partitioned_awinda_gait['hip_angle'] = [reshape_vector(part_strides[trial_type]['hip_angle'][0], new_size = 51), reshape_vector(part_strides[trial_type]['hip_angle'][1], new_size = 51)]\n",
    "        partitioned_awinda_gait['knee_angle'] = [reshape_vector(part_strides[trial_type]['knee_angle'][0], new_size = 51), reshape_vector(part_strides[trial_type]['knee_angle'][1], new_size = 51)]\n",
    "        partitioned_awinda_gait['ankle_angle'] = [reshape_vector(part_strides[trial_type]['ankle_angle'][0], new_size = 51), reshape_vector(part_strides[trial_type]['ankle_angle'][1], new_size = 51)]\n",
    "\n",
    "\n",
    "        # Extract and reshape individual signals\n",
    "        individual_signals = []\n",
    "        gait_scores_list = []\n",
    "\n",
    "\n",
    "        for i in range(partitioned_awinda_gait['pelvis_orient'].shape[0]):\n",
    "            signal_dict = {\n",
    "                'pelvis_orient': partitioned_awinda_gait['pelvis_orient'][i].reshape(1, 51, 3),\n",
    "                'hip_angle': [partitioned_awinda_gait['hip_angle'][0][i].reshape(1, 51, 3), partitioned_awinda_gait['hip_angle'][1][i].reshape(1, 51, 3)],\n",
    "                'knee_angle': [partitioned_awinda_gait['knee_angle'][0][i].reshape(1, 51, 3), partitioned_awinda_gait['knee_angle'][1][i].reshape(1, 51, 3)],\n",
    "                'ankle_angle': [partitioned_awinda_gait['ankle_angle'][0][i].reshape(1, 51, 3), partitioned_awinda_gait['ankle_angle'][1][i].reshape(1, 51, 3)]\n",
    "            }\n",
    "            individual_signals.append(signal_dict)\n",
    "\n",
    "        print(np.shape(individual_signals[0]['pelvis_orient']))    \n",
    "        print(np.shape(individual_signals[0]['hip_angle'][0]))\n",
    "\n",
    "        for signal_val in individual_signals:\n",
    "            gait_scores = calc_gait_profile_score(signal_val, partitioned_awinda_control)\n",
    "            gait_scores_list.append(gait_scores)\n",
    "                \n",
    "        groups, gaitcycles, ordered_strides, ordered_params = check_group_configurations(stance_time_symmetry, flattened_raw_sensor, part_strides, part_gait_params_INI)\n",
    "        \n",
    "        #AB AS CONTROL FOR INI\n",
    "        \n",
    "        participant_info = pd.read_csv(bucket_dir + 'Gait Quality Analysis/Data/Participant_Data/Raw Data/participant_info.csv')\n",
    "        partitioned_gait_data = []\n",
    "        \n",
    "        for index, part_strides in enumerate(ordered_strides):\n",
    "    \n",
    "            params_INI = {\n",
    "                    'stride time':[],\n",
    "                    'stride length':[],\n",
    "                    'swing phase':[],\n",
    "                    'MAV':[],\n",
    "                    'MAH':[],\n",
    "                    'MHD':[],\n",
    "                    'MAB':[],\n",
    "                    'MAD':[],\n",
    "                    'ShROM':[]\n",
    "                    }\n",
    "            \n",
    "            acc_scale = 0.02\n",
    "            partitioned_signals_awinda_grouped = {}\n",
    "            partitioned_signals_awinda_grouped['Pelvis_IMU'] = combine_acc_and_gyro(acc_scale*reshape_vector(part_strides['acc_data'][1]), reshape_vector(part_strides['gyro_data'][1]))\n",
    "            partitioned_signals_awinda_grouped['UpperR_IMU'] = combine_acc_and_gyro(acc_scale*reshape_vector(part_strides['acc_data'][2]), reshape_vector(part_strides['gyro_data'][2]))\n",
    "            partitioned_signals_awinda_grouped['UpperL_IMU'] = combine_acc_and_gyro(acc_scale*reshape_vector(part_strides['acc_data'][5]), reshape_vector(part_strides['gyro_data'][5]))\n",
    "            partitioned_signals_awinda_grouped['LowerR_IMU'] = combine_acc_and_gyro(acc_scale*reshape_vector(part_strides['acc_data'][3]), reshape_vector(part_strides['gyro_data'][3]))\n",
    "            partitioned_signals_awinda_grouped['LowerL_IMU'] = combine_acc_and_gyro(acc_scale*reshape_vector(part_strides['acc_data'][6]), reshape_vector(part_strides['gyro_data'][6]))\n",
    "\n",
    "            partitioned_signals_awinda_grouped['pelvis_orient'] = reshape_vector(part_strides['pelvis_orient'][0], new_size = 51)\n",
    "            partitioned_signals_awinda_grouped['hip_angle'] = [reshape_vector(part_strides['hip_angle'][0], new_size = 51), \n",
    "                                                    reshape_vector(part_strides['hip_angle'][1], new_size = 51)]\n",
    "            partitioned_signals_awinda_grouped['knee_angle'] = [reshape_vector(part_strides['knee_angle'][0], new_size = 51), \n",
    "                                                        reshape_vector(part_strides['knee_angle'][1], new_size = 51)]\n",
    "            partitioned_signals_awinda_grouped['ankle_angle'] = [reshape_vector(part_strides['ankle_angle'][0], new_size = 51), \n",
    "                                                        reshape_vector(part_strides['ankle_angle'][1], new_size = 51)]\n",
    "\n",
    "\n",
    "            indiv_height = participant_info.loc[participant_info['Participant'] == participant]['Height (m)'].item()\n",
    "            for i, param in enumerate(params_INI):\n",
    "                params_INI[param] = ordered_params[index][i]\n",
    "                if(ini_height_norm[param]):\n",
    "                    params_INI[param] = params_INI[param] / indiv_height\n",
    "\n",
    "            partitioned_gait_data.append(AwindaData(trial_type, dict(partitioned_signals_awinda_grouped), dict(params_INI))) \n",
    "\n",
    "        control_INI_params = []\n",
    "        for param in params_INI:\n",
    "            control_INI_params.append(partitioned_control_data.ini_gait_params[param][0])\n",
    "        control_INI_params = np.transpose(np.array(control_INI_params))\n",
    "\n",
    "        mean_control = np.mean(control_INI_params, axis = 0)\n",
    "        std_control = np.std(control_INI_params, axis = 0)\n",
    "\n",
    "        standardized_control = (control_INI_params - mean_control) / std_control\n",
    "        cov_control = np.cov(standardized_control.T)\n",
    "\n",
    "        eig_vals, eig_vecs = np.linalg.eig(cov_control)\n",
    "        print(f\"INI Score for participant: {participant}\")\n",
    "        for i in range(len(partitioned_gait_data)):\n",
    "            side_1 = np.transpose(np.array( [partitioned_gait_data[i].ini_gait_params[param][0] for param in params_INI] ))\n",
    "            side_2 = np.transpose(np.array( [partitioned_gait_data[i].ini_gait_params[param][1] for param in params_INI] ))\n",
    "            test_ini = np.mean([side_1, side_2], axis=0)\n",
    "            ini_data = calc_INI(mean_control, std_control, eig_vals, eig_vecs, test_ini)\n",
    "            ini_score = np.mean(np.linalg.norm(ini_data, axis=-1))\n",
    "            mean_STSR = np.mean(groups[i])\n",
    "            add_row_to_csv(csv_path, 'STSR', 'INI', participant, mean_STSR, ini_score)\n",
    "            #print(mean_STSR)\n",
    "            print(ini_score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\ekuep\\AppData\\Local\\Temp\\ipykernel_18256\\3323904910.py:3: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  script_dir = 'C:\\GP-WearablesAnalysis\\examples'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing participant LLPU_P01\n",
      "Prosthetic on the right side\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P01\n",
      "6.50562763485156\n",
      "8.511521288115723\n",
      "6.896771175108751\n",
      "8.634133148916126\n",
      "7.321034235078354\n",
      "8.766413808585563\n",
      "Processing participant LLPU_P02\n",
      "Prosthetic side on the left side\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P02\n",
      "6.512184340920215\n",
      "9.148542591102428\n",
      "6.860426300371599\n",
      "10.030791250716344\n",
      "7.247605423502377\n",
      "11.205134441382292\n",
      "Processing participant LLPU_P03\n",
      "Prosthetic side on the left side\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P03\n",
      "5.359418524069248\n",
      "7.4306921364707605\n",
      "5.7234976421520765\n",
      "7.987417266759122\n",
      "6.108496371441923\n",
      "8.676826546027538\n",
      "Processing participant LLPU_P04\n",
      "Prosthetic side on the left side\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P04\n",
      "6.764157186620489\n",
      "9.358730905030963\n",
      "7.168498623522591\n",
      "9.380516361518993\n",
      "7.526307469256675\n",
      "9.383592535372099\n",
      "Processing participant LLPU_P05\n",
      "Prosthetic side on the left side\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P05\n",
      "5.281591742117987\n",
      "11.80559318421672\n",
      "5.638320088763972\n",
      "12.378330628376158\n",
      "6.035520750913466\n",
      "13.318497289681726\n",
      "Processing participant LLPU_P06\n",
      "Prosthetic side on the left side\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P06\n",
      "8.53258458733538\n",
      "10.811579025491689\n",
      "8.913704055896911\n",
      "10.949255558456214\n",
      "9.284044581049265\n",
      "10.73382873413285\n",
      "Processing participant LLPU_P08\n",
      "Prosthetic side on the left side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ekuep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P08\n",
      "6.710021000086634\n",
      "9.090562539871886\n",
      "7.070451533809153\n",
      "9.861644064548972\n",
      "7.440923546278977\n",
      "10.634950739910655\n",
      "Processing participant LLPU_P09\n",
      "Prosthetic on the right side\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P09\n",
      "4.638402509864576\n",
      "5.948661227009609\n",
      "4.997930291932548\n",
      "5.892575865497267\n",
      "5.38490319544396\n",
      "5.9675267122596924\n",
      "Processing participant LLPU_P10\n",
      "Prosthetic on the right side\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P10\n",
      "4.706182561868617\n",
      "7.2091388830903815\n",
      "5.072383239942215\n",
      "7.3359189916764755\n",
      "5.469521870929921\n",
      "8.272248207180468\n",
      "Processing participant LLPU_P12\n",
      "Prosthetic on the right side\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P12\n",
      "8.7564638553543\n",
      "15.770494736231262\n",
      "9.122605638590034\n",
      "16.97069415556648\n",
      "9.511039671929431\n",
      "17.69451904896757\n",
      "Processing participant LLPU_P15\n",
      "Prosthetic on the right side\n",
      "(1, 51, 3)\n",
      "(1, 51, 3)\n",
      "INI Score for participant: LLPU_P15\n",
      "6.112090211498027\n",
      "6.838128856963036\n",
      "6.499952093417273\n",
      "6.797399782351133\n",
      "6.881368169125903\n",
      "7.253652280769111\n"
     ]
    }
   ],
   "source": [
    "#Using first baseline group as control for INI \n",
    "\n",
    "script_dir = 'C:\\GP-WearablesAnalysis\\examples'\n",
    "run_time = datetime.datetime.now().strftime(\"%d-%m-%y_%H-%M\")\n",
    "csv_filename = f\"logresults_STSR_INI_BaselineControl{run_time}.csv\" #Builds a log file based on the current time to keep track of runs\n",
    "csv_path = os.path.join(script_dir, csv_filename)\n",
    "\n",
    "def add_row_to_csv(csv_path, gait_param, algorithm, participant_num, level, parameter):\n",
    "    with open(csv_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([csv_path, gait_param, algorithm, participant_num, level, parameter])\n",
    "\n",
    "\n",
    "############## PROCESSING INI\n",
    "\n",
    "XsensGaitParser =  excel_reader.XsensGaitDataParser() #Different excel reader file, which contains the spatiotemporal parameters not pertaining to the INI\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'gaitbfb_propellab/'\n",
    "base_directory = bucket_name + 'Wearable Biofeedback System (REB-0448)/Data/Raw Data'\n",
    "bucket_name = 'gaitbfb_propellab'\n",
    "blobs = storage_client.list_blobs(bucket_name, prefix = base_directory)\n",
    "prefix_from_bucket = 'Wearable Biofeedback System (REB-0448)/Data/Raw Data/' \n",
    "\n",
    "participant_list = ['LLPU_P01','LLPU_P02','LLPU_P03','LLPU_P04','LLPU_P05','LLPU_P06','LLPU_P09','LLPU_P10','LLPU_P12','LLPU_P15']\n",
    "#participant_list = ['LLPU_P01','LLPU_P02','LLPU_P03','LLPU_P04','LLPU_P05','LLPU_P06','LLPU_P09','LLPU_P10','LLPU_P12','LLPU_P15']\n",
    "arrangements = ['pelvis','upper','lower']\n",
    "\n",
    "\n",
    "participant_info_file = pd.read_excel('Q:\\\\main_propellab\\\\Users\\\\Ng, Gabe\\\\Summer Student 2024\\\\LLPU_DataSummaries\\\\LLPU_Height_ProstheticSide.xlsx')\n",
    "\n",
    "def get_participant_info(participant_id):\n",
    "    participant = participant_info_file[participant_info_file['Participant_ID'] == participant_id]\n",
    "    if not participant.empty:\n",
    "        height = participant.iloc[0]['Height']\n",
    "        side = int(participant.iloc[0]['Side'])\n",
    "        return height, side\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "    \n",
    "def organize_signals(sensor_mappings, gyro_signal, accel_signal):\n",
    "    combined_signals = {}\n",
    "    for location, sensor in sensor_mappings.items():\n",
    "        reshaped_gyro = reshape_vector(gyro_signal[sensor], 40, 3)\n",
    "        reshaped_accel = reshape_vector(accel_signal[sensor], 40, 3)\n",
    "        combined_signals[location] = np.concatenate((reshaped_gyro, reshaped_accel), axis=2) #Concatenates to gyro x,y,z and accel x,y,z\n",
    "    return combined_signals\n",
    "\n",
    "def calc_INI(control_mean, control_std, eig_vals, eig_vecs, data):\n",
    "    standardized_data = (data - control_mean) / control_std\n",
    "    transformed_data = (np.dot(standardized_data, eig_vecs)) / np.sqrt(eig_vals)\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "#Dictionary to map the sensor locations to their IDs.\n",
    "sensor_mappings = {\n",
    "    'pelvis': 1,\n",
    "    'UpperR': 2,\n",
    "    'LowerR': 3,\n",
    "    'UpperL': 5,\n",
    "    'LowerL': 6\n",
    "}\n",
    "     \n",
    "for participant in participant_list:\n",
    "    print(f\"Processing participant {participant}\")\n",
    "    \n",
    "    participant_id = participant\n",
    "    height,side = get_participant_info(participant_id)\n",
    "    height_m = height/100 \n",
    "    \n",
    "    if side == 1:\n",
    "        prosth_side = 0 #Right side\n",
    "        non_prosth_side = 1\n",
    "        side_label = \"Right\"\n",
    "        print(\"Prosthetic on the right side\")\n",
    "    else:\n",
    "        prosth_side = 1 #Left side\n",
    "        non_prosth_side = 0\n",
    "        side_label = \"Left\"\n",
    "        print(\"Prosthetic side on the left side\")\n",
    "    \n",
    "    if height is None or side is None:\n",
    "        print(f\"Participant info not found for ID: {participant_id}\")\n",
    "        continue\n",
    "    \n",
    " \n",
    "    directory = prefix_from_bucket + participant + '/Excel_Data_Trimmed'\n",
    "    blobs = storage_client.list_blobs(bucket_or_name=bucket_name, prefix=directory.replace(\"\\\\\", \"/\"))\n",
    "    part_strides = {}\n",
    "    part_gait_params = {}\n",
    "    part_kinematic_params = {}\n",
    "    part_raw_sensor = []\n",
    "    trial_type = 'LLPU'\n",
    "    height_normalized = True\n",
    "    \n",
    "    part_gait_params_INI = {}\n",
    "    part_strides_INI = {}\n",
    "   \n",
    "    logging.info(f\"Processing participant {participant}\")\n",
    "    if blobs:\n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith('.csv'):\n",
    "                try:\n",
    "                    XsensGaitParser.process_mvn_trial_data(f\"gs://{bucket_name}/{blob.name}\")\n",
    "                    partitioned_mvn_data = XsensGaitParser.get_partitioned_mvn_data()\n",
    "                    gait_params = XsensGaitParser.get_gait_param_info()\n",
    "                    combined_signals = organize_signals(sensor_mappings, partitioned_mvn_data['gyro_data'], partitioned_mvn_data['acc_data'])\n",
    "                    pelvis_data = combined_signals['pelvis']\n",
    "                    upper_data = np.concatenate((combined_signals['UpperR'], combined_signals['UpperL']), axis=2)  # Concatenate by last axis\n",
    "                    lower_data = np.concatenate((combined_signals['LowerR'], combined_signals['LowerL']), axis=2)  # Concatenate by last axis\n",
    "                    full_sensors = np.concatenate((pelvis_data,upper_data,lower_data),axis=2)\n",
    "                    part_raw_sensor.append(full_sensors)\n",
    "\n",
    "                    if trial_type in part_strides:\n",
    "                        for body_part in part_strides[trial_type]:\n",
    "                            for i, side in enumerate(part_strides[trial_type][body_part]):\n",
    "                                # for each part (pelvis, l_hip, r_knee, etc.), append strides to appropriate list\n",
    "                                part_strides[trial_type][body_part][i] = part_strides[trial_type][body_part][i] + partitioned_mvn_data[body_part][i]\n",
    "                        \n",
    "                        part_gait_params[trial_type].append(gait_params['spatio_temp'])\n",
    "                        \n",
    "                        for joint in part_kinematic_params[trial_type]:\n",
    "                            for i, side in enumerate(part_kinematic_params[trial_type][joint]):\n",
    "                                part_kinematic_params[trial_type][joint][i] = np.append(part_kinematic_params[trial_type][joint][i], gait_params['kinematics'][joint][i], axis=0) \n",
    "\n",
    "                    else:\n",
    "                        part_strides[trial_type] = partitioned_mvn_data\n",
    "                        part_kinematic_params[trial_type] = gait_params['kinematics']\n",
    "                        part_gait_params[trial_type] = [gait_params['spatio_temp']]\n",
    "                    \n",
    "                    file_name = os.path.basename(blob.name)\n",
    "                    \n",
    "                    ##CHANGE THE excel reader - access the different \n",
    "                    XsensGaitParser_GN =  excel_reader_GN.XsensGaitDataParser()\n",
    "                    XsensGaitParser_GN.process_mvn_trial_data(f\"gs://{bucket_name}/{blob.name}\")\n",
    "                    partitioned_mvn_data_GN = XsensGaitParser_GN.get_partitioned_mvn_data()\n",
    "                    gait_params_GN = XsensGaitParser_GN.get_gait_param_info()\n",
    "                    \n",
    "                    if trial_type in part_strides_INI:\n",
    "                        for body_part in part_strides[trial_type]:\n",
    "                            for i, side in enumerate(part_strides_INI[trial_type][body_part]):\n",
    "                                # for each part (pelvis, l_hip, r_knee, etc.), append strides to appropriate list\n",
    "                                part_strides_INI[trial_type][body_part][i] = part_strides_INI[trial_type][body_part][i] + partitioned_mvn_data_GN[body_part][i]\n",
    "                        \n",
    "                        for j, param in enumerate(part_gait_params_INI[trial_type]):\n",
    "                            part_gait_params_INI[trial_type][j] = np.concatenate((part_gait_params_INI[trial_type][j], gait_params_GN['spatio_temp'][j]), axis=-1)\n",
    "\n",
    "                    else:\n",
    "                        part_strides_INI[trial_type] = partitioned_mvn_data_GN\n",
    "                        part_gait_params_INI[trial_type] = gait_params_GN['spatio_temp']\n",
    "                    \n",
    "\n",
    "                except IndexError as e: #Exception based on an Index Error encountered in excel_reader_gcp.py **\n",
    "                    #print(f\"File skipped: gs://{bucket_name}/{blob.name} due to error: {e}\")\n",
    "                    continue                              \n",
    "    \n",
    "    if trial_type in part_gait_params:\n",
    "        \n",
    "        stance_time_symmetry = [item for sublist in [i[11] for i in part_gait_params[trial_type]] for item in sublist]\n",
    "        \n",
    "        partitioned_awinda_gait = {}\n",
    "        partitioned_awinda_gait['pelvis_orient'] = reshape_vector(part_strides[trial_type]['pelvis_orient'][0], new_size = 51)\n",
    "        partitioned_awinda_gait['hip_angle'] = [reshape_vector(part_strides[trial_type]['hip_angle'][0], new_size = 51), reshape_vector(part_strides[trial_type]['hip_angle'][1], new_size = 51)]\n",
    "        partitioned_awinda_gait['knee_angle'] = [reshape_vector(part_strides[trial_type]['knee_angle'][0], new_size = 51), reshape_vector(part_strides[trial_type]['knee_angle'][1], new_size = 51)]\n",
    "        partitioned_awinda_gait['ankle_angle'] = [reshape_vector(part_strides[trial_type]['ankle_angle'][0], new_size = 51), reshape_vector(part_strides[trial_type]['ankle_angle'][1], new_size = 51)]\n",
    "\n",
    "        # Extract and reshape individual signals\n",
    "        individual_signals = []\n",
    "        gait_scores_list = []\n",
    "\n",
    "        for i in range(partitioned_awinda_gait['pelvis_orient'].shape[0]):\n",
    "            signal_dict = {\n",
    "                'pelvis_orient': partitioned_awinda_gait['pelvis_orient'][i].reshape(1, 51, 3),\n",
    "                'hip_angle': [partitioned_awinda_gait['hip_angle'][0][i].reshape(1, 51, 3), partitioned_awinda_gait['hip_angle'][1][i].reshape(1, 51, 3)],\n",
    "                'knee_angle': [partitioned_awinda_gait['knee_angle'][0][i].reshape(1, 51, 3), partitioned_awinda_gait['knee_angle'][1][i].reshape(1, 51, 3)],\n",
    "                'ankle_angle': [partitioned_awinda_gait['ankle_angle'][0][i].reshape(1, 51, 3), partitioned_awinda_gait['ankle_angle'][1][i].reshape(1, 51, 3)]\n",
    "            }\n",
    "            individual_signals.append(signal_dict)\n",
    "\n",
    "        print(np.shape(individual_signals[0]['pelvis_orient']))    \n",
    "        print(np.shape(individual_signals[0]['hip_angle'][0]))\n",
    "\n",
    "        for signal_val in individual_signals:\n",
    "            gait_scores = calc_gait_profile_score(signal_val, partitioned_awinda_control)\n",
    "            gait_scores_list.append(gait_scores)\n",
    "            \n",
    "        flattened_raw_sensor = []\n",
    "        part_strides_list = []\n",
    "        for sublist in part_raw_sensor:\n",
    "            for item in sublist:\n",
    "                flattened_raw_sensor.append(item) #Flatten to individual gait cycles\n",
    "                \n",
    "        groups, gaitcycles, ordered_strides, ordered_params = check_group_configurations(stance_time_symmetry, flattened_raw_sensor, part_strides, part_gait_params_INI)\n",
    "        \n",
    "        #Establishing control as the first group\n",
    "        participant_info = pd.read_csv(bucket_dir + 'Gait Quality Analysis/Data/Participant_Data/Raw Data/participant_info.csv')\n",
    "        partitioned_gait_data = []\n",
    "        \n",
    "    \n",
    "        for index, part_strides in enumerate(ordered_strides):\n",
    "            \n",
    "            params_INI = {\n",
    "                'stride time': [],\n",
    "                'stride length': [],\n",
    "                'swing phase': [],\n",
    "                'MAV': [],\n",
    "                'MAH': [],\n",
    "                'MHD': [],\n",
    "                'MAB': [],\n",
    "                'MAD': [],\n",
    "                'ShROM': []\n",
    "            }\n",
    "            \n",
    "            acc_scale = 0.02\n",
    "            partitioned_signals_awinda_grouped = {\n",
    "                'Pelvis_IMU': combine_acc_and_gyro(acc_scale * reshape_vector(part_strides['acc_data'][1]), reshape_vector(part_strides['gyro_data'][1])),\n",
    "                'UpperR_IMU': combine_acc_and_gyro(acc_scale * reshape_vector(part_strides['acc_data'][2]), reshape_vector(part_strides['gyro_data'][2])),\n",
    "                'UpperL_IMU': combine_acc_and_gyro(acc_scale * reshape_vector(part_strides['acc_data'][5]), reshape_vector(part_strides['gyro_data'][5])),\n",
    "                'LowerR_IMU': combine_acc_and_gyro(acc_scale * reshape_vector(part_strides['acc_data'][3]), reshape_vector(part_strides['gyro_data'][3])),\n",
    "                'LowerL_IMU': combine_acc_and_gyro(acc_scale * reshape_vector(part_strides['acc_data'][6]), reshape_vector(part_strides['gyro_data'][6])),\n",
    "                'pelvis_orient': reshape_vector(part_strides['pelvis_orient'][0], new_size=51),\n",
    "                'hip_angle': [reshape_vector(part_strides['hip_angle'][0], new_size=51), reshape_vector(part_strides['hip_angle'][1], new_size=51)],\n",
    "                'knee_angle': [reshape_vector(part_strides['knee_angle'][0], new_size=51), reshape_vector(part_strides['knee_angle'][1], new_size=51)],\n",
    "                'ankle_angle': [reshape_vector(part_strides['ankle_angle'][0], new_size=51), reshape_vector(part_strides['ankle_angle'][1], new_size=51)]\n",
    "            }\n",
    "\n",
    "            indiv_height = participant_info.loc[participant_info['Participant'] == participant]['Height (m)'].item()\n",
    "            \n",
    "            for i, param in enumerate(params_INI):\n",
    "                params_INI[param] = ordered_params[index][i]\n",
    "                if ini_height_norm[param]:\n",
    "                    params_INI[param] = params_INI[param] / indiv_height\n",
    "\n",
    "            # Check if the current index is 0 (control group)\n",
    "            if index == 0:\n",
    "                partitioned_control_data = AwindaData('control', dict(partitioned_signals_awinda_grouped), dict(params_INI))\n",
    "\n",
    "            else:\n",
    "                # Append to the regular partitioned_gait_data list\n",
    "                partitioned_gait_data.append(AwindaData(trial_type, dict(partitioned_signals_awinda_grouped), dict(params_INI)))\n",
    "\n",
    "        control_INI_params = []\n",
    "        \n",
    "        for param in params_INI:\n",
    "            control_INI_params.append(partitioned_control_data.ini_gait_params[param][prosth_side])\n",
    "        control_INI_params = np.transpose(np.array(control_INI_params))\n",
    "\n",
    "        mean_control = np.mean(control_INI_params, axis = 0)\n",
    "        std_control = np.std(control_INI_params, axis = 0)\n",
    "\n",
    "        standardized_control = (control_INI_params - mean_control) / std_control\n",
    "        cov_control = np.cov(standardized_control.T)\n",
    "        \n",
    "        eig_vals, eig_vecs = np.linalg.eig(cov_control)\n",
    "        print(f\"INI Score for participant: {participant}\")\n",
    "        for i in range(len(partitioned_gait_data)):\n",
    "            side_1 = np.transpose(np.array( [partitioned_gait_data[i].ini_gait_params[param][0] for param in params_INI] ))\n",
    "            side_2 = np.transpose(np.array( [partitioned_gait_data[i].ini_gait_params[param][1] for param in params_INI] ))\n",
    "            test_ini = np.mean([side_1, side_2], axis=0)\n",
    "            ini_data = calc_INI(mean_control, std_control, eig_vals, eig_vecs, test_ini)\n",
    "            ini_score = np.mean(np.linalg.norm(ini_data, axis=-1))\n",
    "            mean_STSR = np.mean(groups[i+1])\n",
    "            add_row_to_csv(csv_path, 'STSR', 'INI', participant, mean_STSR, ini_score)\n",
    "            print(mean_STSR)\n",
    "            print(ini_score)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(9, 2, 100)\n",
      "(9, 2, 50)\n",
      "(9, 2, 50)\n",
      "(9, 2, 50)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(len(ordered_params))\n",
    "\n",
    "for group in ordered_params:\n",
    "    print(np.shape(group))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
