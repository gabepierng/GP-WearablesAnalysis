{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script for processing the GPS scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import scipy.interpolate as interp\n",
    "sys.path.append(os.path.join(sys.path[0], '..', 'src'))\n",
    "import excel_reader_gcp as excel_reader\n",
    "import gait_metrics as gait_metrics\n",
    "from gait_metrics import *\n",
    "import datetime\n",
    "import logging   \n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "b20, a20 = scipy.signal.butter(N=4, Wn = 0.8, btype = 'lowpass')  # Wn = 0.8 = 40 / Nyquist F = 50Hz\n",
    "run_time = datetime.datetime.now().strftime(\"%d-%m-%y_%H-%M\")\n",
    "# script_dir = os.path.dirname(__file__)\n",
    "# current_datetime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "# csv_filename = f\"logresults_{run_time}.csv\" #Builds a log file based on the current time to keep track of runs\n",
    "# csv_path = os.path.join(script_dir, csv_filename)\n",
    "\n",
    "\n",
    "def reshape_vector(vectors_orig, new_size, num_axes=3):\n",
    "    x_new = np.linspace(0, 100, new_size)\n",
    "    trial_reshaped = []\n",
    "    for stride in vectors_orig:\n",
    "        x_orig = np.linspace(0, 100, len(stride))\n",
    "        func_cubic = [interp.interp1d(x_orig, stride[:, i], kind='cubic') for i in range(num_axes)]\n",
    "        vec_cubic = np.array([func_cubic[i](x_new) for i in range(num_axes)]).transpose()\n",
    "        trial_reshaped.append(vec_cubic)\n",
    "    return np.array(trial_reshaped)\n",
    "\n",
    "#uses dictionaries to extract the relevant raw sensor data, reshapes the data, then concatenates gyro and accelerometer signals together \n",
    "def organize_signals(sensor_mappings, gyro_signal, accel_signal):\n",
    "    combined_signals = {}\n",
    "    for location, sensor in sensor_mappings.items():\n",
    "        reshaped_gyro = reshape_vector(gyro_signal[sensor], 40, 3)\n",
    "        reshaped_accel = reshape_vector(accel_signal[sensor], 40, 3)\n",
    "        combined_signals[location] = np.concatenate((reshaped_gyro, reshaped_accel), axis=2) #Concatenates to gyro x,y,z and accel x,y,z\n",
    "    return combined_signals\n",
    "\n",
    "\n",
    "storage_client = storage.Client()\n",
    "part_strides = {}\n",
    "part_gait_params = {}\n",
    "part_kinematic_params = {}\n",
    "control_strides = {}\n",
    "control_gait_params = {}\n",
    "control_kinematic_params = {}\n",
    "\n",
    "\n",
    "bucket_dir = 'gs://gaitbfb_propellab/'\n",
    "def compile_gait_data(store_gait_cycles, store_gait_params, store_kin_params, filenames, trial_type_filter, print_filenames=False, look_at_all_files = True, desired_filetypes=None):   \n",
    "\n",
    "    XsensGaitParser = excel_reader.XsensGaitDataParser()  \n",
    "    for i, file in enumerate(sorted(filenames)):\n",
    "        trial_type = re.search(trial_type_filter, file).group(1)\n",
    "        if(look_at_all_files or any(filetype in file for filetype in desired_filetypes)):\n",
    "            XsensGaitParser.process_mvn_trial_data(os.path.join(bucket_dir, file))\n",
    "            partitioned_mvn_data = XsensGaitParser.get_partitioned_mvn_data()\n",
    "            gait_params = XsensGaitParser.get_gait_param_info()\n",
    "\n",
    "            if trial_type in store_gait_cycles:\n",
    "                for body_part in store_gait_cycles[trial_type]:\n",
    "                    for i, side in enumerate(store_gait_cycles[trial_type][body_part]):\n",
    "                        # for each part (pelvis, l_hip, r_knee, etc.), append strides to appropriate list\n",
    "                        store_gait_cycles[trial_type][body_part][i] = store_gait_cycles[trial_type][body_part][i] + partitioned_mvn_data[body_part][i]\n",
    "\n",
    "                store_gait_params[trial_type].append(gait_params['spatio_temp'])\n",
    "\n",
    "                for joint in store_kin_params[trial_type]:\n",
    "                    for i, side in enumerate(store_kin_params[trial_type][joint]):\n",
    "                        store_kin_params[trial_type][joint][i] = np.append(store_kin_params[trial_type][joint][i], gait_params['kinematics'][joint][i], axis=0)\n",
    "\n",
    "            else:\n",
    "                store_gait_cycles[trial_type] = partitioned_mvn_data\n",
    "                store_gait_params[trial_type] = [gait_params['spatio_temp']]\n",
    "                store_kin_params[trial_type] = gait_params['kinematics']\n",
    "\n",
    "\n",
    "bucket_name = 'gaitbfb_propellab/'\n",
    "base_directory = bucket_name + 'Wearable Biofeedback System (REB-0448)/Data/Raw Data'\n",
    "bucket_name = 'gaitbfb_propellab'\n",
    "prefix = 'control_dir'\n",
    "control_dir = 'Gait Quality Analysis/Data/Participant_Data/Processed Data/AbleBodied_Control/CSV'\n",
    "blobs = storage_client.list_blobs(bucket_name, prefix = control_dir)\n",
    "control_files = []\n",
    "for blob in blobs:\n",
    "    if('.csv' in blob.name):\n",
    "        control_files.append(blob.name)\n",
    "\n",
    "compile_gait_data(control_strides, control_gait_params, control_kinematic_params, control_files, 'CSV/(.*?)-00')        \n",
    "\n",
    "aggregate_control_data = {}\n",
    "strides_per_control = 10\n",
    "for i, indiv in enumerate(control_strides.keys()):\n",
    "    indices = np.arange(len(control_strides[indiv]['gyro_data'][0]))\n",
    "    np.random.shuffle(indices)\n",
    "    #control_strides_per_part.append(min(strides_per_control, len(indices)))\n",
    "    \n",
    "    if(i == 0):\n",
    "        aggregate_control_data = control_strides[indiv]\n",
    "        \n",
    "        for signal_type in control_strides[indiv]:\n",
    "            for j, side in enumerate(control_strides[indiv][signal_type]):\n",
    "                aggregate_control_data[signal_type][j] = [control_strides[indiv][signal_type][j][indices[k]] for k in range(min(strides_per_control, len(indices))) ]\n",
    "                    \n",
    "                    \n",
    "    else:\n",
    "        # randomly sample 10 gait cycles from each able-bodied in control, or all gait cycles if less than 10\n",
    "        for signal_type in control_strides[indiv]:\n",
    "            for j, side in enumerate(control_strides[indiv][signal_type]):\n",
    "                aggregate_control_data[signal_type][j] = aggregate_control_data[signal_type][j] + [control_strides[indiv][signal_type][j][indices[k]] \n",
    "                                                                                                for k in range(min(strides_per_control, len(indices))) ]\n",
    "\n",
    "# reshape all the kinematic signals to the size specified for the GPS (51, e.g. 2% increments across the gait cycles from HS to HS)\n",
    "# store in partitioned_awinda_control\n",
    "partitioned_awinda_control = {}\n",
    "partitioned_awinda_control['pelvis_orient'] = reshape_vector(aggregate_control_data['pelvis_orient'][0], new_size = 51)\n",
    "partitioned_awinda_control['hip_angle'] = [reshape_vector(aggregate_control_data['hip_angle'][0], new_size = 51), reshape_vector(aggregate_control_data['hip_angle'][1], new_size = 51)]\n",
    "partitioned_awinda_control['knee_angle'] = [reshape_vector(aggregate_control_data['knee_angle'][0], new_size = 51), reshape_vector(aggregate_control_data['knee_angle'][1], new_size = 51)]\n",
    "partitioned_awinda_control['ankle_angle'] = [reshape_vector(aggregate_control_data['ankle_angle'][0], new_size = 51), reshape_vector(aggregate_control_data['ankle_angle'][1], new_size = 51)]\n",
    "\n",
    "\n",
    "def calculate_state_correspondence_matrix(hmm_1, hmm_2, n_states):\n",
    "    def calculate_stationary_distribution(hmm):\n",
    "        eigenvals, eigenvectors = np.linalg.eig(hmm.model.transmat_.T)\n",
    "        stationary = np.array(eigenvectors[:, np.where(np.abs(eigenvals - 1.) < 1e-8)[0][0]])\n",
    "        stationary = stationary / np.sum(stationary)\n",
    "        return np.expand_dims(stationary.real, axis=-1)\n",
    "\n",
    "    # KL-Divergence = method for determining the difference between two probability distributions\n",
    "    def calculate_KL_div(hmm_model_1, hmm_model_2, state_model_1, state_model_2):\n",
    "        means_1 = np.expand_dims(hmm_model_1.means_[state_model_1], axis=-1)\n",
    "        means_2 = np.expand_dims(hmm_model_2.means_[state_model_2], axis=-1)\n",
    "\n",
    "        covars_1 = hmm_model_1.covars_[state_model_1]\n",
    "        covars_2 = hmm_model_2.covars_[state_model_2]\n",
    "\n",
    "        term_1 = (means_2 - means_1).T @ np.linalg.inv(covars_2) @ (means_2 - means_1)\n",
    "        term_2 = np.trace(np.linalg.inv(covars_2) @ covars_1)\n",
    "        term_3 = np.log(np.linalg.det(covars_1) / np.linalg.det(covars_2))\n",
    "        term_4 = len(covars_1)\n",
    "\n",
    "        kl_divergence = 0.5 * (term_1 + term_2 - term_3 - term_4)\n",
    "\n",
    "        return kl_divergence\n",
    "\n",
    "    kl_state_comparisons = np.zeros((n_states, n_states))\n",
    "    pi_1 = calculate_stationary_distribution(hmm_1)\n",
    "    pi_2 = calculate_stationary_distribution(hmm_2)\n",
    "    total_expected_similarity = 0\n",
    "\n",
    "    for i in range(n_states):\n",
    "        for j in range(n_states):\n",
    "            kl_state_comparisons[i,j] = 0.5 * (calculate_KL_div(hmm_1.model, hmm_2.model, i, j) + calculate_KL_div(hmm_2.model, hmm_1.model, i, j))\n",
    "            total_expected_similarity = total_expected_similarity + (pi_1[i] * pi_2[j] * kl_state_comparisons[i,j])\n",
    "\n",
    "    k = 1\n",
    "\n",
    "    # alternative methods of calculating similarity based on KL-Divergence\n",
    "    # s_e = np.exp(-k * kl_state_comparisons)\n",
    "    s_e = 1 / kl_state_comparisons\n",
    "\n",
    "    # pi_1.T @ pi_2 should produce a N x N matrix (pi_1i * pi_2j)\n",
    "    q_matrix = ((pi_1 @ pi_2.T) * s_e) / total_expected_similarity\n",
    "\n",
    "    return q_matrix\n",
    "\n",
    "def calculate_gini_index(q_matrix, n_states):\n",
    "    def calc_gini(vector):\n",
    "        vector = np.sort(vector)\n",
    "        l1_norm = np.linalg.norm(vector, 1)\n",
    "        a = 0\n",
    "        for i in range(1, n_states+1):\n",
    "            a = a + (vector[i-1] / l1_norm) * ((n_states - i + 0.5) / (n_states - 1))\n",
    "\n",
    "        vec_sparsity = (n_states / (n_states - 1)) - (2 * a)\n",
    "\n",
    "        return vec_sparsity\n",
    "\n",
    "    # calculate mean sparsity of each row vector (r) and column vector (c) in Q matrix\n",
    "    r = (1 / n_states) * np.sum([calc_gini(row) for row in q_matrix])\n",
    "    c = (1 / n_states) * np.sum([calc_gini(column) for column in q_matrix.T])\n",
    "\n",
    "    gini_index = 0.5 * (r + c)\n",
    "    return gini_index\n",
    "\n",
    "# Function to add a row of data to the CSV file\n",
    "def add_row_to_csv(csv_path, sensor_config, gait_param, algorithm, participant_num, level, parameter):\n",
    "    with open(csv_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([csv_path, sensor_config, gait_param, algorithm, participant_num, level, parameter])\n",
    "\n",
    "#uses dictionaries to extract the relevant raw sensor data, reshapes the data, then concatenates gyro and accelerometer signals together \n",
    "def organize_signals(sensor_mappings, gyro_signal, accel_signal):\n",
    "    combined_signals = {}\n",
    "    for location, sensor in sensor_mappings.items():\n",
    "        reshaped_gyro = reshape_vector(gyro_signal[sensor], 40, 3)\n",
    "        reshaped_accel = reshape_vector(accel_signal[sensor], 40, 3)\n",
    "        combined_signals[location] = np.concatenate((reshaped_gyro, reshaped_accel), axis=2) #Concatenates to gyro x,y,z and accel x,y,z\n",
    "    return combined_signals\n",
    "\n",
    "\"\"\" Data grouping/splitting pipeline:\"\"\"\n",
    "\n",
    "\"\"\" Split the data into desired number of groups, and extract the first \"target mean\" from the first group. Subsequent target means are calculated as X% (percent grading) away from previous groups\n",
    "    ex. if 3% is selected, means of groups will increment by 3% - the direction of incrementing (up/down) depends on whether reverse is selected \n",
    "    Iterate through indices in the gait parameter data (in this case, STSR) and append to a grouping if they are within a threshold from the target mean. \n",
    "    Sort both the gait parameter and corresponding gait cycles based on the chosen indices. \n",
    "\"\"\"\n",
    "\n",
    "def finding_groupings(num_groups, gait_parameter, gait_cycles, percent_grading, reverse=True):\n",
    "    \n",
    "    if reverse:\n",
    "        percent_grading = -percent_grading\n",
    "        values_sorted = sorted(gait_parameter, reverse=True)\n",
    "        sorted_indices = np.argsort(gait_parameter)[::-1]  # Sort indices in descending order of stance time symmetry\n",
    "    else:\n",
    "        sorted_indices = np.argsort(gait_parameter)\n",
    "        values_sorted = sorted(gait_parameter, reverse=False)\n",
    "\n",
    "    n = len(gait_parameter)\n",
    "    group_sizes = [n // num_groups + (1 if i < n % num_groups else 0) for i in range(num_groups)]\n",
    "    target_means = [np.mean(values_sorted[:group_sizes[0]])]\n",
    "\n",
    "    # Initializes the groups and the remaining values to be picked from\n",
    "    groups = [[] for _ in range(num_groups)]\n",
    "    grouped_gait_cycles = [[] for _ in range(num_groups)]\n",
    "    remaining_indices = sorted_indices[:]\n",
    "\n",
    "    for i in range(1, num_groups):\n",
    "        target_means.append(target_means[0] + percent_grading * i)\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        target_mean = target_means[i]\n",
    "        filtered_indices = [idx for idx in remaining_indices if abs(gait_parameter[idx] - target_mean) < percent_grading / 2]\n",
    "        selected_indices = filtered_indices[:]\n",
    "        groups[i].extend(gait_parameter[idx] for idx in selected_indices)\n",
    "        grouped_gait_cycles[i].extend(gait_cycles[idx] for idx in selected_indices)\n",
    "        remaining_indices = [idx for idx in remaining_indices if idx not in selected_indices]\n",
    "\n",
    "    return groups, grouped_gait_cycles, percent_grading            \n",
    "\n",
    "\"\"\" Random Sampling Gait Cycles:\"\"\"\n",
    "\n",
    "\"\"\" random_sampling: Randomly sample 50 gait cycles from group 1. This serves as the \"first_mean\" that will be compared to in adaptive subsampling.\n",
    "    adaptive_subsample: Randomly sample 50 gait cycles from a given group. If it is within X% * i +/- tolerance (i is the index of the group (ex. group 2 = 1)), then that group is accepted.\n",
    "    Otherwise, the maximum or minimum is removed and another value still available is added (depending on if the current percent difference is too high or too low)\n",
    "    Handles if the means are decreasing or increasing (if percent_diff is negative or positive)\n",
    "    Returns the indices of the groups, and these are used to update the new groups in random_sampling.\n",
    "\"\"\"\n",
    "\n",
    "def random_sampling(groups, grouped_gait_cycles, sample_size=50):\n",
    "    def adaptive_subsample(group, first_mean, i, percent_grading=0.4, tolerance=0.5, sample_size=50, max_iterations=10000):\n",
    "        available_indices = list(range(len(group)))  # Make a list that spans all the indices\n",
    "        sample_indices = np.random.choice(available_indices, size=sample_size, replace=False)\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            available_indices.remove(idx)  # Remove initial sample values from available values\n",
    "        for _ in range(max_iterations):\n",
    "            current_mean = np.mean([group[idx] for idx in sample_indices])\n",
    "            percent_diff = current_mean - first_mean \n",
    "            target_diff = percent_grading * i\n",
    "\n",
    "            if len(available_indices) == 0:\n",
    "                raise ValueError(\"No candidates available to adjust the mean\")\n",
    "            \n",
    "            if (target_diff - tolerance) <= abs(percent_diff) <= (target_diff + tolerance):\n",
    "                return sample_indices\n",
    "            \n",
    "            if abs(percent_diff) < (target_diff - tolerance):\n",
    "                if percent_diff < 0:\n",
    "                    # Choose a new sample from the lower half\n",
    "                    lower_idx = [idx for idx in available_indices if group[idx] <= np.percentile(group, 50)]\n",
    "                    if lower_idx:\n",
    "                        new_idx = np.random.choice(lower_idx)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    else:\n",
    "                        new_idx = np.random.choice(available_indices)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    \n",
    "                    available_indices.remove(new_idx)\n",
    "                    sample_indices = np.delete(sample_indices, np.argmax([group[idx] for idx in sample_indices]))\n",
    "                else:\n",
    "                    # Choose a new sample from the upper half\n",
    "                    higher_idx = [idx for idx in available_indices if group[idx] >= np.percentile(group, 50)]\n",
    "                    if higher_idx:\n",
    "                        new_idx = np.random.choice(higher_idx)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    else:\n",
    "                        new_idx = np.random.choice(available_indices)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    \n",
    "                    available_indices.remove(new_idx)\n",
    "                    sample_indices = np.delete(sample_indices, np.argmin([group[idx] for idx in sample_indices]))\n",
    "            else:\n",
    "                if percent_diff > 0:\n",
    "                    lower_idx = [idx for idx in available_indices if group[idx] <= np.percentile(group, 50)]\n",
    "                    if lower_idx:\n",
    "                        new_idx = np.random.choice(lower_idx)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    else:\n",
    "                        new_idx = np.random.choice(available_indices)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    \n",
    "                    available_indices.remove(new_idx)\n",
    "                    sample_indices = np.delete(sample_indices, np.argmax([group[idx] for idx in sample_indices]))\n",
    "                else:\n",
    "                    # Choose a new sample from the upper half\n",
    "                    higher_idx = [idx for idx in available_indices if group[idx] >= np.percentile(group, 50)]\n",
    "                    if higher_idx:\n",
    "                        new_idx = np.random.choice(lower_idx)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    else:\n",
    "                        new_idx = np.random.choice(available_indices)\n",
    "                        sample_indices = np.append(sample_indices, new_idx)\n",
    "                    \n",
    "                    available_indices.remove(new_idx)\n",
    "                    sample_indices = np.delete(sample_indices, np.argmin([group[idx] for idx in sample_indices]))\n",
    "\n",
    "        raise ValueError(\"Could not find suitable subsample within the maximum number of iterations\")\n",
    "\n",
    "    indices_first_group = list(range(len(groups[0])))  \n",
    "    sample_indices_first_group = np.random.choice(indices_first_group, size=sample_size*2, replace=False)\n",
    "    group1_mean = np.mean([groups[0][idx] for idx in sample_indices_first_group]) # first mean used as the target for all subsequent groups\n",
    "    \n",
    "    random.shuffle(sample_indices_first_group)\n",
    "    baseline_1_indices = sample_indices_first_group[:50]\n",
    "    baseline_2_indices = sample_indices_first_group[50:]\n",
    "    \n",
    "    subsampled_values_baseline1 = [groups[0][j] for j in baseline_1_indices]\n",
    "    subsampled_gait_cycles_baseline1 = [grouped_gait_cycles[0][j] for j in baseline_1_indices]\n",
    "    \n",
    "    subsampled_values_baseline2 = [groups[0][j] for j in baseline_2_indices]\n",
    "    subsampled_gait_cycles_baseline2 = [grouped_gait_cycles[0][j] for j in baseline_2_indices]\n",
    "    \n",
    "    groups_subsampled_list = []\n",
    "    gaitcycles_subsampled_list = []\n",
    "    \n",
    "    groups_subsampled_list.append(subsampled_values_baseline1)\n",
    "    groups_subsampled_list.append(subsampled_values_baseline2)\n",
    "    \n",
    "    gaitcycles_subsampled_list.append(subsampled_gait_cycles_baseline1)\n",
    "    gaitcycles_subsampled_list.append(subsampled_gait_cycles_baseline2)\n",
    "    \n",
    "    #Only consider 3 groups here\n",
    "    for i in range(1, 3):\n",
    "        sample_indices = adaptive_subsample(np.array(groups[i]), group1_mean, i)\n",
    "        subsampled_values = [groups[i][j] for j in sample_indices]\n",
    "        subsampled_gait_cycles = [grouped_gait_cycles[i][j] for j in sample_indices]\n",
    "        groups_subsampled_list.append(subsampled_values)\n",
    "        gaitcycles_subsampled_list.append(subsampled_gait_cycles)\n",
    "    \n",
    "    return groups_subsampled_list, gaitcycles_subsampled_list\n",
    "\n",
    "\"\"\" Group splitting and sampling are called. Checks to see which direction the grouping should be done in, and only appends the groups that have at least 70 points\"\"\"\n",
    "\n",
    "def check_group_configurations(gait_split_parameter, raw_sensor_data):\n",
    "    percent_grading = 0.4\n",
    "    groups, grouped_gait_cycles, grading = finding_groupings(3, gait_split_parameter, raw_sensor_data, percent_grading, reverse=False)\n",
    "    \n",
    "    filtered_groups = []\n",
    "    filtered_gait_groups = []\n",
    "    \n",
    "    for i in range(len(groups)):\n",
    "        if len(groups[i]) > 70:\n",
    "            filtered_groups.append(groups[i])\n",
    "            filtered_gait_groups.append(grouped_gait_cycles[i])\n",
    "    \n",
    "    if len(filtered_groups) < 3:\n",
    "        groups, grouped_gait_cycles, grading = finding_groupings(3, gait_split_parameter, raw_sensor_data, percent_grading, reverse=True)  # Try the other direction if requirements are not fulfilled\n",
    "        filtered_groups = []\n",
    "        filtered_gait_groups = []\n",
    "        \n",
    "        for i in range(len(groups)):\n",
    "            if len(groups[i]) > 70:\n",
    "                filtered_groups.append(groups[i])\n",
    "                filtered_gait_groups.append(grouped_gait_cycles[i])\n",
    "\n",
    "        if len(filtered_groups) < 3:\n",
    "            raise ValueError(\"Insufficient group sizes available for this participant\")\n",
    "    \n",
    "    groups, gaitcycles = random_sampling(filtered_groups, filtered_gait_groups)\n",
    "    \n",
    "    return groups, gaitcycles\n",
    "\n",
    "\n",
    "\"\"\"Main section of code for processing participants\"\"\"\n",
    "\n",
    "#Dictionary to map the sensor locations to their IDs.\n",
    "sensor_mappings = {\n",
    "    'pelvis': 1,\n",
    "    'UpperR': 2,\n",
    "    'LowerR': 3,\n",
    "    'UpperL': 5,\n",
    "    'LowerL': 6\n",
    "}\n",
    "\n",
    "XsensGaitParser =  excel_reader.XsensGaitDataParser()\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'gaitbfb_propellab/'\n",
    "base_directory = bucket_name + 'Wearable Biofeedback System (REB-0448)/Data/Raw Data'\n",
    "bucket_name = 'gaitbfb_propellab'\n",
    "blobs = storage_client.list_blobs(bucket_name, prefix = base_directory)\n",
    "prefix_from_bucket = 'Wearable Biofeedback System (REB-0448)/Data/Raw Data/' \n",
    "\n",
    "participant_list = ['LLPU_P01','LLPU_P02','LLPU_P03','LLPU_P04','LLPU_P05','LLPU_P06','LLPU_P08','LLPU_P09','LLPU_P10','LLPU_P12','LLPU_P14','LLPU_P15']\n",
    "\n",
    "arrangements = ['pelvis','upper','lower']\n",
    "\n",
    "STSR_full = []\n",
    "StepLength_full = []\n",
    "GPS_full = []\n",
    "\n",
    "participant_info = pd.read_excel('Q:\\\\main_propellab\\\\Users\\\\Ng, Gabe\\\\Summer Student 2024\\\\LLPU_DataSummaries\\\\LLPU_Height_ProstheticSide.xlsx')\n",
    "\n",
    "def get_participant_info(participant_id):\n",
    "    participant = participant_info[participant_info['Participant_ID'] == participant_id]\n",
    "    if not participant.empty:\n",
    "        height = participant.iloc[0]['Height']\n",
    "        side = int(participant.iloc[0]['Side'])\n",
    "        return height, side\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "#Dictionary to map the sensor locations to their IDs.\n",
    "sensor_mappings = {\n",
    "    'pelvis': 1,\n",
    "    'UpperR': 2,\n",
    "    'LowerR': 3,\n",
    "    'UpperL': 5,\n",
    "    'LowerL': 6\n",
    "}\n",
    "     \n",
    "for participant in participant_list:\n",
    "    print(f\"Processing participant {participant}\")\n",
    "    \n",
    "    participant_id = participant\n",
    "    height,side = get_participant_info(participant_id)\n",
    "    height_m = height/100 \n",
    "    \n",
    "    if side == 1:\n",
    "        prosth_side = 0 #Right side\n",
    "        non_prosth_side = 1\n",
    "        side_label = \"Right\"\n",
    "        print(\"Prosthetic on the right side\")\n",
    "    else:\n",
    "        prosth_side = 1 #Left side\n",
    "        non_prosth_side = 0\n",
    "        side_label = \"Left\"\n",
    "        print(\"Prosthetic side on the left side\")\n",
    "    \n",
    "    if height is None or side is None:\n",
    "        print(f\"Participant info not found for ID: {participant_id}\")\n",
    "        continue\n",
    "    \n",
    "    directory = prefix_from_bucket + participant + '/Excel_Data_Trimmed'\n",
    "    blobs = storage_client.list_blobs(bucket_or_name=bucket_name, prefix=directory.replace(\"\\\\\", \"/\"))\n",
    "    part_strides = {}\n",
    "    part_gait_params = {}\n",
    "    part_kinematic_params = {}\n",
    "    part_raw_sensor = []\n",
    "    trial_type = 'LLPU'\n",
    "    height_normalized = True\n",
    "    \n",
    "    part_strides_baseline = {}\n",
    "    part_gait_params_baseline = {}\n",
    "    part_kinematic_params_baseline = {}\n",
    "    part_sensor_data_baseline = []\n",
    "    \n",
    "    knee_roms_list = []\n",
    "    hip_roms_list = []\n",
    "    step_lengths_list = []\n",
    "    ankle_roms_list = []\n",
    "   \n",
    "    logging.info(f\"Processing participant {participant}\")\n",
    "    if blobs:\n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith('.csv'):\n",
    "                try:\n",
    "                    XsensGaitParser.process_mvn_trial_data(f\"gs://{bucket_name}/{blob.name}\")\n",
    "                    partitioned_mvn_data = XsensGaitParser.get_partitioned_mvn_data()\n",
    "                    gait_params = XsensGaitParser.get_gait_param_info()\n",
    "                    combined_signals = organize_signals(sensor_mappings, partitioned_mvn_data['gyro_data'], partitioned_mvn_data['acc_data'])\n",
    "                    pelvis_data = combined_signals['pelvis']\n",
    "                    upper_data = np.concatenate((combined_signals['UpperR'], combined_signals['UpperL']), axis=2)  # Concatenate by last axis\n",
    "                    lower_data = np.concatenate((combined_signals['LowerR'], combined_signals['LowerL']), axis=2)  # Concatenate by last axis\n",
    "                    full_sensors = np.concatenate((pelvis_data,upper_data,lower_data),axis=2)\n",
    "                    part_raw_sensor.append(full_sensors)\n",
    "\n",
    "                    if trial_type in part_strides:\n",
    "                        for body_part in part_strides[trial_type]:\n",
    "                            for i, side in enumerate(part_strides[trial_type][body_part]):\n",
    "                                # for each part (pelvis, l_hip, r_knee, etc.), append strides to appropriate list\n",
    "                                part_strides[trial_type][body_part][i] = part_strides[trial_type][body_part][i] + partitioned_mvn_data[body_part][i]\n",
    "                               \n",
    "                        part_gait_params[trial_type].append(gait_params['spatio_temp'])\n",
    "                        \n",
    "                        for joint in part_kinematic_params[trial_type]:\n",
    "                            for i, side in enumerate(part_kinematic_params[trial_type][joint]):\n",
    "                                part_kinematic_params[trial_type][joint][i] = np.append(part_kinematic_params[trial_type][joint][i], gait_params['kinematics'][joint][i], axis=0) \n",
    "\n",
    "                    else:\n",
    "                        part_strides[trial_type] = partitioned_mvn_data\n",
    "                        part_gait_params[trial_type] = [gait_params['spatio_temp']]\n",
    "                        part_kinematic_params[trial_type] = gait_params['kinematics']\n",
    "                    file_name = os.path.basename(blob.name)\n",
    "\n",
    "                except IndexError as e: #Exception based on an Index Error encountered in excel_reader_gcp.py **\n",
    "                    #print(f\"File skipped: gs://{bucket_name}/{blob.name} due to error: {e}\")\n",
    "                    continue                              \n",
    "    \n",
    "    if trial_type in part_gait_params:\n",
    "    \n",
    "        partitioned_awinda_gait = {}\n",
    "        partitioned_awinda_gait['pelvis_orient'] = reshape_vector(part_strides[trial_type]['pelvis_orient'][0], new_size = 51)\n",
    "        partitioned_awinda_gait['hip_angle'] = [reshape_vector(part_strides[trial_type]['hip_angle'][0], new_size = 51), reshape_vector(part_strides[trial_type]['hip_angle'][1], new_size = 51)]\n",
    "        partitioned_awinda_gait['knee_angle'] = [reshape_vector(part_strides[trial_type]['knee_angle'][0], new_size = 51), reshape_vector(part_strides[trial_type]['knee_angle'][1], new_size = 51)]\n",
    "        partitioned_awinda_gait['ankle_angle'] = [reshape_vector(part_strides[trial_type]['ankle_angle'][0], new_size = 51), reshape_vector(part_strides[trial_type]['ankle_angle'][1], new_size = 51)]\n",
    "\n",
    "\n",
    "        # Extract and reshape individual signals\n",
    "        individual_signals = []\n",
    "        gait_scores_list = []\n",
    "\n",
    "\n",
    "        for i in range(partitioned_awinda_gait['pelvis_orient'].shape[0]):\n",
    "            signal_dict = {\n",
    "                'pelvis_orient': partitioned_awinda_gait['pelvis_orient'][i].reshape(1, 51, 3),\n",
    "                'hip_angle': [partitioned_awinda_gait['hip_angle'][0][i].reshape(1, 51, 3), partitioned_awinda_gait['hip_angle'][1][i].reshape(1, 51, 3)],\n",
    "                'knee_angle': [partitioned_awinda_gait['knee_angle'][0][i].reshape(1, 51, 3), partitioned_awinda_gait['knee_angle'][1][i].reshape(1, 51, 3)],\n",
    "                'ankle_angle': [partitioned_awinda_gait['ankle_angle'][0][i].reshape(1, 51, 3), partitioned_awinda_gait['ankle_angle'][1][i].reshape(1, 51, 3)]\n",
    "            }\n",
    "            individual_signals.append(signal_dict)\n",
    "\n",
    "        print(np.shape(individual_signals[0]['pelvis_orient']))    \n",
    "        print(np.shape(individual_signals[0]['hip_angle'][0]))\n",
    "\n",
    "        for signal_val in individual_signals:\n",
    "            gait_scores = calc_gait_profile_score(signal_val, partitioned_awinda_control)\n",
    "            gait_scores_list.append(gait_scores)\n",
    "\n",
    "        GPS_full.append(gait_scores_list)\n",
    "        \n",
    "        flattened_raw_sensor = []\n",
    "        for sublist in part_raw_sensor:\n",
    "            for item in sublist:\n",
    "                flattened_raw_sensor.append(item) #Flatten to individual gait cycles \n",
    "        \n",
    "        ordered_groups, ordered_gaitcycles = check_group_configurations(gait_scores_list, flattened_raw_sensor)\n",
    "        ordered_group_means = [np.mean(group) for group in ordered_groups]\n",
    "        group_lengths = [len(group) for group in ordered_groups]\n",
    "        print(group_lengths)\n",
    "        print(ordered_group_means)\n",
    "    \n",
    "        for k, group in enumerate(ordered_groups):\n",
    "            print(f\"Group {k+1}: {len(group)} (Mean: {np.mean(group) if group else 'N/A'})\")\n",
    "        \n",
    "        \"\"\"Splitting of the raw sensor data was done with all of the sensors concatenated along the last axis (i.e. each gait cycle with 40 points would be a 40x30 array (6 axis pelvis + 12 axis upper + 12 axis lower))\"\"\"\n",
    "        \"\"\"This is used to split them out into their respective sensor configurations (pelvis, upper, lower)\"\"\"\n",
    "        \n",
    "        # Split each array and append to respective lists\n",
    "        gaitcycles_40x6 = [] #pelvis\n",
    "        gaitcycles_40x12_1 = [] #upper\n",
    "        gaitcycles_40x12_2 = [] #lower\n",
    "\n",
    "        # Iterate over each sublist in gaitcycles\n",
    "        for sublist in ordered_gaitcycles:\n",
    "            sublist_40x6 = []\n",
    "            sublist_40x12_1 = []\n",
    "            sublist_40x12_2 = []\n",
    "            \n",
    "            # Split each array in the sublist\n",
    "            for array in sublist:\n",
    "                split_arrays = np.split(array, [6, 18], axis=1)  # Split the array into 40x6, 40x12, 40x12 parts\n",
    "                sublist_40x6.append(split_arrays[0]) #Filter individual gait cycles and split them (first 6, next 12, next 12)\n",
    "                sublist_40x12_1.append(split_arrays[1])\n",
    "                sublist_40x12_2.append(split_arrays[2])\n",
    "            \n",
    "            # Append the split sublists to the main lists\n",
    "            gaitcycles_40x6.append(sublist_40x6)\n",
    "            gaitcycles_40x12_1.append(sublist_40x12_1)\n",
    "            gaitcycles_40x12_2.append(sublist_40x12_2)\n",
    "\n",
    "        # Pelvis, upper, lower\n",
    "        combined_sensor_configs = [gaitcycles_40x6, gaitcycles_40x12_1, gaitcycles_40x12_2]            \n",
    "        \n",
    "        for sensor_idx, raw_sensor in enumerate(combined_sensor_configs): #Iterates through each sensor configuration (pelvis, upper, lower)\n",
    "            \n",
    "            # \"\"\" DTW Implementation\"\"\"\n",
    "            print(f\"Sensor arrangement: {arrangements[sensor_idx]}\")\n",
    "            dtw_mean_distances = []\n",
    "            #Computing the within group distance for baseline\n",
    "            dtw_within = tslearn_dtw_analysis(set1 = raw_sensor[0], set2=None) # type: ignore\n",
    "            dtw_mean_distances.append(dtw_within)\n",
    "            #add_row_to_csv(csv_path, arrangements[sensor_idx],'GPS', 'DTW',participant, ordered_group_means[0], dtw_within)\n",
    "            \n",
    "            for j in range(1, len(raw_sensor)):\n",
    "                dtw_between = tslearn_dtw_analysis(set1 = raw_sensor[0], set2 = raw_sensor[j]) # type: ignore\n",
    "                dtw_mean_distances.append(dtw_between)\n",
    "                #add_row_to_csv(csv_path, arrangements[sensor_idx],'GPS', 'DTW',participant, ordered_group_means[j], dtw_between)\n",
    "                \n",
    "            print(dtw_mean_distances) \n",
    "            \n",
    "            # \"\"\" SOM Implementation\"\"\"\n",
    "            # Shuffle and split the list\n",
    "            train_arrays, test_arrays = train_test_split(raw_sensor[0], test_size=0.2, random_state=42)\n",
    "            train_data = np.concatenate(train_arrays, axis=0)\n",
    "            test_data = np.concatenate(test_arrays, axis=0)\n",
    "        \n",
    "            print(\"Training Data Shape:\", train_data.shape)\n",
    "            #print(\"Testing Data Shape:\", test_data.shape)\n",
    "            print(\"Training the SOM on baseline data\")\n",
    "            MDP_mean_deviations = []\n",
    "            trained_SOM = train_minisom(train_data, learning_rate=0.1, topology='hexagonal', normalize=True) # type: ignore\n",
    "            test_baseline = calculate_MDP(test_data, train_data, trained_SOM, normalize=True) # type: ignore\n",
    "            MDP_mean_deviations.append(np.mean(test_baseline))\n",
    "            #add_row_to_csv(csv_path, arrangements[sensor_idx],'GPS','MDP',participant, ordered_group_means[0], np.mean(test_baseline))\n",
    "            \n",
    "            \n",
    "            for j in range(1, len(ordered_gaitcycles)):\n",
    "                #Shuffle and split the list (only looking at the test data here)\n",
    "                train_arrays, test_arrays = train_test_split(raw_sensor[j], test_size=0.2, random_state=42)\n",
    "                test_data_upperlevels = np.concatenate(test_arrays, axis=0)\n",
    "                test_upperlevels = calculate_MDP(test_data_upperlevels, train_data, trained_SOM, normalize=True) # type: ignore\n",
    "                MDP_mean_deviations.append(np.mean(test_upperlevels))\n",
    "                #add_row_to_csv(csv_path, arrangements[sensor_idx],'GPS','MDP',participant, ordered_group_means[j], np.mean(test_upperlevels))\n",
    "            \n",
    "            print(f\"MDP mean deviations: {MDP_mean_deviations}\")\n",
    "            \n",
    "            #Implementing the HMM\n",
    "            strides_train_flat = {}\n",
    "            strides_test_flat = {}\n",
    "            strides_train = {}\n",
    "            strides_test = {}\n",
    "            resize_len = 40\n",
    "            strides_to_concat = 10\n",
    "            num_states= 3 #Changed from 5 to 2 #Test 3 as well \n",
    "            train_iterations = 300 \n",
    "            train_tolerance = 1e-2\n",
    "            num_models_train = 10\n",
    "            concat_strides = {}\n",
    "   \n",
    "            for idx, group in enumerate(raw_sensor):\n",
    "                concat_strides[idx] = []\n",
    "                group = np.array(group)\n",
    "                for i in range(group.shape[0] - strides_to_concat):\n",
    "                    temp = []\n",
    "                    for j in range(strides_to_concat):\n",
    "                        temp.append(group[i + j])\n",
    "                    concat_strides[idx].append(np.concatenate(temp, axis=0))\n",
    "\n",
    "                concat_strides[idx] = np.array(concat_strides[idx])\n",
    "                concat_strides[idx] = signal.filtfilt(b20, a20, concat_strides[idx], axis=1)\n",
    "                \n",
    "            hmm_models = {}  \n",
    "                \n",
    "            for idx, group in enumerate(raw_sensor):\n",
    "                \n",
    "                num_models_training = num_models_train #Re initialize the number of models for training \n",
    "                hmm_models[idx] = []\n",
    "                \n",
    "                for j in range(num_models_training):\n",
    "                    train_forward_model = True\n",
    "                    k = 0\n",
    "                    \n",
    "                    while(train_forward_model):\n",
    "                        print('Train Attempt ', k+1, end=\"\\r\", flush=True)\n",
    "                        \n",
    "                        if(j > -1):\n",
    "                            np.random.shuffle(concat_strides[idx])\n",
    "                            \n",
    "                        # flatten sequence for hmmlearn train function\n",
    "                        strides_sequence_flattened = concat_strides[idx].reshape((concat_strides[idx].shape[0] * concat_strides[idx].shape[1], -1))\n",
    "                        \n",
    "                        # technically is no training/testing data, but this preserves a few gait cycles to compare the hidden-state sequence predictions of the HMMs\n",
    "                        len_train = int(0.95 * len(concat_strides[idx]))\n",
    "                        strides_train[idx] = concat_strides[idx][:len_train]\n",
    "                        strides_test[idx] = concat_strides[idx][len_train:]\n",
    "                        sequence_length = resize_len * strides_to_concat\n",
    "                        strides_train_flat[idx] = strides_sequence_flattened[:sequence_length * len_train]\n",
    "                        strides_test_flat[idx] = strides_sequence_flattened[sequence_length * len_train:]\n",
    "\n",
    "                        hmm_model = HMMTrainer(n_components = num_states, n_iter = train_iterations, tolerance = train_tolerance)\n",
    "                        hmm_model.train(strides_train_flat[idx], sequence_length, len_train)\n",
    "\n",
    "\n",
    "                        # double checks for left-to-right architecture in transition matrix\n",
    "                        valid_rows = 0\n",
    "                        a_mat = hmm_model.model.transmat_\n",
    "                        for i, row in enumerate(a_mat):\n",
    "                            temp = np.argpartition(np.roll(row, -i), -2)[-2:]\n",
    "                            if((np.array(temp) == np.array([0,1])).all() or (np.array(temp) == np.array([1,0])).all()):\n",
    "                                valid_rows = valid_rows + 1\n",
    "\n",
    "                        # correct_second_state = [i for i in range(num_states - 1)]\n",
    "                        # correct_second_state.append(0)        \n",
    "                        # for i, row in enumerate(hmm_model.model.transmat_):\n",
    "                        #     max_state = np.argmax(row)\n",
    "                        #     if(max_state == i):\n",
    "                        #         temp = [j for j in row if not (j == row[max_state])]\n",
    "                        #         if(np.argmax(temp) == correct_second_state[i]):\n",
    "                        #             valid_rows = valid_rows + 1\n",
    "\n",
    "                        #if model is left-to-right, consider model trained, train next model (until num_models_train reached)\n",
    "                        \n",
    "                        if(valid_rows == num_states):\n",
    "                            train_forward_model = False\n",
    "                        k = k + 1\n",
    "                    hmm_models[idx].append(hmm_model)\n",
    "\n",
    "            print('done')\n",
    "            \n",
    "            test_predict = strides_test[0][1] #2nd element of idx = 0\n",
    "            min_predict = np.min(test_predict[:,1])\n",
    "            max_predict = np.max(test_predict[:,1])\n",
    "            \n",
    "            \n",
    "            def align_states(trained_hmm_model, roll_amount=0):\n",
    "                new_hmm = copy.deepcopy(trained_hmm_model)\n",
    "                array_order = np.roll(np.arange(num_states), roll_amount)\n",
    "\n",
    "                new_hmm.model.transmat_ = new_hmm.model.transmat_[array_order,: ]\n",
    "                for i, row in enumerate(new_hmm.model.transmat_):\n",
    "                    new_hmm.model.transmat_[i] = np.roll(new_hmm.model.transmat_[i], roll_amount)\n",
    "\n",
    "                new_hmm.model.means_ = new_hmm.model.means_[array_order, :]\n",
    "                new_hmm.model.covars_ = new_hmm.model.covars_[array_order, :]\n",
    "                new_hmm.model.startprob_ = new_hmm.model.startprob_[array_order]\n",
    "                return new_hmm \n",
    "\n",
    "            pred_vals = np.ones(num_states)\n",
    "            \n",
    "            for state in range(num_states):\n",
    "                pred_vals[state] = min_predict + ((state * (max_predict - min_predict)) / (num_states - 1))\n",
    "        \n",
    "            roll_amounts = {}\n",
    "            match_trials = {}\n",
    "\n",
    "            for idx, group in enumerate(raw_sensor):\n",
    "                roll_amounts[idx] = [0 for i in range(num_models_train)]\n",
    "                match_trials[idx] = 0\n",
    "\n",
    "            shift_all = 0\n",
    "        \n",
    "            def find_best_alignment(hmm_1, hmm_2, test_stride, n_states):\n",
    "                min_distance = 9999999\n",
    "                best_roll = 0\n",
    "\n",
    "                for j in range(n_states):\n",
    "                    new_hmm = align_states(hmm_2, j)\n",
    "                    prediction_1 = hmm_1.model.predict(test_stride)\n",
    "                    prediction_2 = new_hmm.model.predict(test_stride)\n",
    "\n",
    "                    distance = np.sum((prediction_1 - prediction_2) ** 2)\n",
    "                    if (distance < min_distance):\n",
    "                        min_distance = distance\n",
    "                        best_roll = j\n",
    "\n",
    "                return best_roll\n",
    "\n",
    "            predictions = {}\n",
    "            hmm_models_aligned_states = {}\n",
    "            \n",
    "            for idx, group in enumerate(raw_sensor):\n",
    "                \n",
    "                predictions[idx] = []\n",
    "                hmm_models_aligned_states[idx] = []\n",
    "                match_trials[idx] = find_best_alignment(hmm_models[0][0], hmm_models[idx][0], test_predict, num_states)\n",
    "                \n",
    "                roll_amounts[idx] = [0] * len(hmm_models[idx]) #Can now handle different lengths of trained models \n",
    "                \n",
    "                for j in range(len(hmm_models[idx])):\n",
    "                    roll_amounts[idx][j] = find_best_alignment(hmm_models[idx][0], hmm_models[idx][j], test_predict, num_states) + match_trials[idx]\n",
    "                    # roll_amounts[idx][j] = 0\n",
    "                    hmm_models_aligned_states[idx].append(align_states(hmm_models[idx][j], roll_amounts[idx][j] + shift_all))\n",
    "                    predictions[idx].append(hmm_models_aligned_states[idx][-1].model.predict(test_predict))\n",
    "                        \n",
    "            \n",
    "                #bunch of stuff for visualizing the hidden-state sequence predictions\n",
    "\n",
    "                #print(roll_amounts[trial_types[1]][0])\n",
    "                fig, ax = plt.subplots()\n",
    "                fig.set_size_inches(12,8)\n",
    "                plt.plot(test_predict[:,0])\n",
    "\n",
    "                plt.plot([pred_vals[j] for j in predictions[0]][0], 'k')\n",
    "                plt.plot([pred_vals[j] for j in predictions[0]][4], 'r')\n",
    "\n",
    "                # plt.plot([pred_vals[j] for j in predictions_post[4]], 'k')\n",
    "                # plt.plot([pred_vals[j] for j in predictions_post[2]], 'r')\n",
    "\n",
    "                # def trial_avg_and_CI(signal_set):\n",
    "                #     conf_int_mult = 1.00    # confidence interval multiplier for 1 std\n",
    "\n",
    "                #     avg_signal = np.mean(signal_set, axis=0)\n",
    "                #     std_signal = np.std(signal_set, axis=0)\n",
    "                #     upper_bound = avg_signal + (conf_int_mult * std_signal)\n",
    "                #     lower_bound = avg_signal - (conf_int_mult * std_signal)\n",
    "\n",
    "                #     return avg_signal, upper_bound, lower_bound\n",
    "\n",
    "                # def confidence_plot(plot_signals, fig_ax, trial_num):\n",
    "                #     plot_signals = trial_avg_and_CI(plot_signals)\n",
    "                #     x = np.arange(len(plot_signals[0]))\n",
    "                #     fig_ax.plot(x, plot_signals[0], color=plot_colors[trial_num])\n",
    "                #     fig_ax.fill_between(x, plot_signals[1], plot_signals[2], color=plot_colors[trial_num], alpha=0.2)\n",
    "\n",
    "                # textstr = '\\n'.join((\n",
    "                #     r'$tolerance=%.5f$' % (train_tolerance, ),\n",
    "                #     r'$iterations=%d$' % (train_iterations, ),\n",
    "                #     r'$states=%d$' % (num_states, )))\n",
    "                # props = dict(boxstyle='round', facecolor='wheat', alpha=1)\n",
    "                # ax.text(1.05, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n",
    "                #         verticalalignment='top', bbox=props)\n",
    "\n",
    "                #plt.show()\n",
    "\n",
    "            symmranges = []\n",
    "            mean_difs = []\n",
    "                    \n",
    "            if num_models_train == 1:\n",
    "                for j, group in enumerate(raw_sensor):\n",
    "                    sum_dif = 0\n",
    "                    count = 0\n",
    "                    if (j == 0):\n",
    "                        m = 1 #only want the other trained model\n",
    "                    else:\n",
    "                        m = 0\n",
    "                    x = calculate_state_correspondence_matrix(hmm_models_aligned_states[0][0], hmm_models_aligned_states[j][m], num_states)\n",
    "                    sum_dif = sum_dif + calculate_gini_index(x, num_states)\n",
    "                    # log HMM-SM similarity\n",
    "                    print('%s - %s  :  %.5f' % (ordered_group_means[0], ordered_group_means[j], sum_dif))\n",
    "                    symmrange = '%s - %s' % (ordered_group_means[0], ordered_group_means[j])\n",
    "                    #add_row_to_csv(csv_path, arrangements[sensor_idx],'GPS','HMM-SM',participant, symmrange, sum_dif)\n",
    "                    \n",
    "            else: #If averaging a larger set (multiple HMMs trained)\n",
    "                # i and j iterate over the trial types.\n",
    "                # compare all permutations between HMMs in trial_types[i] and trial_types[j] to compute a mean HMM-SM similarity\n",
    "                # between the two symmetry ranges. If i and j are the same (e.g., comparing within a symmetry range), don't compare\n",
    "                # HMM to itself\n",
    "                for j, group in enumerate(raw_sensor):\n",
    "                    sum_dif = 0\n",
    "                    count = 0\n",
    "                    for k in range(num_models_train):\n",
    "                        if(j == 0): #Always comparing to the first group, so if j (other group is same as baseline, make sure it is testing against )\n",
    "                            indices = [a for a in range(num_models_train) if (not a == k)]\n",
    "                        else:\n",
    "                            indices = np.arange(num_models_train)\n",
    "                        for m in indices:\n",
    "                            x = calculate_state_correspondence_matrix(hmm_models_aligned_states[0][k], hmm_models_aligned_states[j][m], num_states)\n",
    "                            sum_dif = sum_dif + calculate_gini_index(x, num_states)\n",
    "                            count = count+1\n",
    "\n",
    "                    # log average HMM-SM similarity\n",
    "                    mean_dif = sum_dif / count\n",
    "                    print('%s - %s  :  %.5f' % (ordered_group_means[0], ordered_group_means[j], mean_dif))\n",
    "                    symmrange = '%s - %s' % (round(ordered_group_means[0],3), round(ordered_group_means[j],3))\n",
    "                    symmranges.append(symmrange)\n",
    "                    mean_difs.append(mean_dif)\n",
    "                    #add_row_to_csv(csv_path, arrangements[sensor_idx],'GPS','HMM-SM',participant, symmrange, mean_dif)\n",
    "                    \n",
    "                \n",
    "                \n",
    "                     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
